{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGqurumVU8rds3Il8gHvLK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-Ty-Dwou88a",
        "outputId": "a437ab31-f86f-4091-aa9a-6b7b972bb0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuML available - GPU acceleration enabled\n",
            "Using device: cuda\n",
            "Loading data...\n",
            "Processing training sequences...\n",
            "Preparing training data for model...\n",
            "Binary class distribution: [3038 5113]\n",
            "Multiclass class distribution: [638 637 161 638 640 161 640 640 161 640 640 161 477 161 640 478 477 161]\n",
            "Number of unique multiclass labels: 18\n",
            "Class imbalance ratio: 0.5941717191472716\n",
            "Enhanced binary class weights: [1.         0.89125758]\n",
            "Enhanced multiclass class weights shape: (18,)\n",
            "Multiclass weights range: 0.708 - 2.813\n",
            "Validation binary distribution: [ 607 1024]\n",
            "Loading test data...\n",
            "Processing test sequences...\n",
            "Preparing test data for model...\n",
            "\n",
            "PyTorch Model Summary:\n",
            "Parameters: 728,531\n",
            "Trainable parameters: 728,531\n",
            "Starting precision-optimized model training...\n",
            "\n",
            "Epoch 1/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 0: Updated optimal threshold to 0.4925 (F1: 0.8502, Precision: 0.8141)\n",
            "Train Loss: 2.0387, Val Loss: 1.9970\n",
            "Train Binary Acc: 0.6235, Val Binary Acc: 0.8007\n",
            "Train Multiclass Acc: 0.0860, Val Multiclass Acc: 0.1931\n",
            "Val Precision@Recall: 0.7464\n",
            "\n",
            "Epoch 2/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.9849, Val Loss: 1.9284\n",
            "Train Binary Acc: 0.7337, Val Binary Acc: 0.8118\n",
            "Train Multiclass Acc: 0.1663, Val Multiclass Acc: 0.2244\n",
            "Val Precision@Recall: 0.7609\n",
            "\n",
            "Epoch 3/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.9338, Val Loss: 1.8734\n",
            "Train Binary Acc: 0.7762, Val Binary Acc: 0.8228\n",
            "Train Multiclass Acc: 0.2074, Val Multiclass Acc: 0.2612\n",
            "Val Precision@Recall: 0.7842\n",
            "\n",
            "Epoch 4/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 3: Updated optimal threshold to 0.4633 (F1: 0.8814, Precision: 0.8325)\n",
            "Train Loss: 1.9002, Val Loss: 1.8521\n",
            "Train Binary Acc: 0.7910, Val Binary Acc: 0.8424\n",
            "Train Multiclass Acc: 0.2356, Val Multiclass Acc: 0.2673\n",
            "Val Precision@Recall: 0.8083\n",
            "\n",
            "Epoch 5/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8888, Val Loss: 1.8260\n",
            "Train Binary Acc: 0.8055, Val Binary Acc: 0.8565\n",
            "Train Multiclass Acc: 0.2439, Val Multiclass Acc: 0.2931\n",
            "Val Precision@Recall: 0.8157\n",
            "\n",
            "Epoch 6/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8700, Val Loss: 1.8236\n",
            "Train Binary Acc: 0.8132, Val Binary Acc: 0.8553\n",
            "Train Multiclass Acc: 0.2640, Val Multiclass Acc: 0.2992\n",
            "Val Precision@Recall: 0.8247\n",
            "\n",
            "Epoch 7/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 6: Updated optimal threshold to 0.4633 (F1: 0.8889, Precision: 0.8515)\n",
            "Train Loss: 1.8640, Val Loss: 1.8101\n",
            "Train Binary Acc: 0.8242, Val Binary Acc: 0.8547\n",
            "Train Multiclass Acc: 0.2672, Val Multiclass Acc: 0.3084\n",
            "Val Precision@Recall: 0.8239\n",
            "\n",
            "Epoch 8/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8520, Val Loss: 1.7978\n",
            "Train Binary Acc: 0.8296, Val Binary Acc: 0.8510\n",
            "Train Multiclass Acc: 0.2796, Val Multiclass Acc: 0.3188\n",
            "Val Precision@Recall: 0.8328\n",
            "\n",
            "Epoch 9/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8441, Val Loss: 1.7911\n",
            "Train Binary Acc: 0.8299, Val Binary Acc: 0.8663\n",
            "Train Multiclass Acc: 0.2873, Val Multiclass Acc: 0.3342\n",
            "Val Precision@Recall: 0.8454\n",
            "\n",
            "Epoch 10/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 9: Updated optimal threshold to 0.4656 (F1: 0.9043, Precision: 0.8742)\n",
            "Train Loss: 1.8366, Val Loss: 1.7736\n",
            "Train Binary Acc: 0.8270, Val Binary Acc: 0.8706\n",
            "Train Multiclass Acc: 0.2916, Val Multiclass Acc: 0.3427\n",
            "Val Precision@Recall: 0.8426\n",
            "\n",
            "Epoch 11/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8291, Val Loss: 1.7732\n",
            "Train Binary Acc: 0.8339, Val Binary Acc: 0.8645\n",
            "Train Multiclass Acc: 0.3057, Val Multiclass Acc: 0.3391\n",
            "Val Precision@Recall: 0.8644\n",
            "\n",
            "Epoch 12/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8212, Val Loss: 1.7594\n",
            "Train Binary Acc: 0.8331, Val Binary Acc: 0.8780\n",
            "Train Multiclass Acc: 0.3161, Val Multiclass Acc: 0.3605\n",
            "Val Precision@Recall: 0.8646\n",
            "\n",
            "Epoch 13/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 12: Updated optimal threshold to 0.4687 (F1: 0.9127, Precision: 0.8788)\n",
            "Train Loss: 1.8127, Val Loss: 1.7675\n",
            "Train Binary Acc: 0.8402, Val Binary Acc: 0.8811\n",
            "Train Multiclass Acc: 0.3183, Val Multiclass Acc: 0.3489\n",
            "Val Precision@Recall: 0.8774\n",
            "\n",
            "Epoch 14/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.8056, Val Loss: 1.7589\n",
            "Train Binary Acc: 0.8465, Val Binary Acc: 0.8670\n",
            "Train Multiclass Acc: 0.3259, Val Multiclass Acc: 0.3556\n",
            "Val Precision@Recall: 0.8486\n",
            "\n",
            "Epoch 15/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7997, Val Loss: 1.7449\n",
            "Train Binary Acc: 0.8428, Val Binary Acc: 0.8853\n",
            "Train Multiclass Acc: 0.3334, Val Multiclass Acc: 0.3740\n",
            "Val Precision@Recall: 0.8797\n",
            "\n",
            "Epoch 16/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7958, Val Loss: 1.7425\n",
            "Train Binary Acc: 0.8429, Val Binary Acc: 0.8719\n",
            "Train Multiclass Acc: 0.3324, Val Multiclass Acc: 0.3685\n",
            "Val Precision@Recall: 0.8555\n",
            "\n",
            "Epoch 17/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7940, Val Loss: 1.7319\n",
            "Train Binary Acc: 0.8528, Val Binary Acc: 0.8749\n",
            "Train Multiclass Acc: 0.3293, Val Multiclass Acc: 0.3850\n",
            "Val Precision@Recall: 0.8565\n",
            "\n",
            "Epoch 18/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7915, Val Loss: 1.7218\n",
            "Train Binary Acc: 0.8482, Val Binary Acc: 0.8927\n",
            "Train Multiclass Acc: 0.3387, Val Multiclass Acc: 0.3869\n",
            "Val Precision@Recall: 0.8837\n",
            "\n",
            "Epoch 19/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 18: Updated optimal threshold to 0.4582 (F1: 0.9144, Precision: 0.8820)\n",
            "Train Loss: 1.7905, Val Loss: 1.7301\n",
            "Train Binary Acc: 0.8491, Val Binary Acc: 0.8872\n",
            "Train Multiclass Acc: 0.3376, Val Multiclass Acc: 0.3857\n",
            "Val Precision@Recall: 0.8791\n",
            "\n",
            "Epoch 20/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7764, Val Loss: 1.7253\n",
            "Train Binary Acc: 0.8526, Val Binary Acc: 0.8829\n",
            "Train Multiclass Acc: 0.3512, Val Multiclass Acc: 0.3869\n",
            "Val Precision@Recall: 0.8854\n",
            "\n",
            "Epoch 21/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7739, Val Loss: 1.7281\n",
            "Train Binary Acc: 0.8532, Val Binary Acc: 0.8817\n",
            "Train Multiclass Acc: 0.3567, Val Multiclass Acc: 0.3838\n",
            "Val Precision@Recall: 0.8728\n",
            "\n",
            "Epoch 22/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7658, Val Loss: 1.7160\n",
            "Train Binary Acc: 0.8551, Val Binary Acc: 0.8878\n",
            "Train Multiclass Acc: 0.3630, Val Multiclass Acc: 0.3998\n",
            "Val Precision@Recall: 0.8657\n",
            "\n",
            "Epoch 23/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7592, Val Loss: 1.7069\n",
            "Train Binary Acc: 0.8575, Val Binary Acc: 0.8896\n",
            "Train Multiclass Acc: 0.3727, Val Multiclass Acc: 0.4010\n",
            "Val Precision@Recall: 0.8734\n",
            "\n",
            "Epoch 24/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7514, Val Loss: 1.7119\n",
            "Train Binary Acc: 0.8610, Val Binary Acc: 0.8884\n",
            "Train Multiclass Acc: 0.3799, Val Multiclass Acc: 0.3985\n",
            "Val Precision@Recall: 0.8870\n",
            "\n",
            "Epoch 25/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 24: Updated optimal threshold to 0.4729 (F1: 0.9184, Precision: 0.8879)\n",
            "Train Loss: 1.7535, Val Loss: 1.6967\n",
            "Train Binary Acc: 0.8644, Val Binary Acc: 0.8921\n",
            "Train Multiclass Acc: 0.3758, Val Multiclass Acc: 0.4249\n",
            "Val Precision@Recall: 0.8879\n",
            "\n",
            "Epoch 26/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7539, Val Loss: 1.7028\n",
            "Train Binary Acc: 0.8586, Val Binary Acc: 0.8939\n",
            "Train Multiclass Acc: 0.3730, Val Multiclass Acc: 0.4071\n",
            "Val Precision@Recall: 0.8845\n",
            "\n",
            "Epoch 27/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7494, Val Loss: 1.6943\n",
            "Train Binary Acc: 0.8640, Val Binary Acc: 0.8976\n",
            "Train Multiclass Acc: 0.3764, Val Multiclass Acc: 0.4194\n",
            "Val Precision@Recall: 0.8951\n",
            "\n",
            "Epoch 28/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 27: Updated optimal threshold to 0.4814 (F1: 0.9240, Precision: 0.8941)\n",
            "Train Loss: 1.7379, Val Loss: 1.6938\n",
            "Train Binary Acc: 0.8649, Val Binary Acc: 0.9001\n",
            "Train Multiclass Acc: 0.3896, Val Multiclass Acc: 0.4157\n",
            "Val Precision@Recall: 0.8968\n",
            "\n",
            "Epoch 29/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7351, Val Loss: 1.6852\n",
            "Train Binary Acc: 0.8687, Val Binary Acc: 0.9013\n",
            "Train Multiclass Acc: 0.3923, Val Multiclass Acc: 0.4280\n",
            "Val Precision@Recall: 0.8960\n",
            "\n",
            "Epoch 30/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7366, Val Loss: 1.6852\n",
            "Train Binary Acc: 0.8667, Val Binary Acc: 0.8982\n",
            "Train Multiclass Acc: 0.3899, Val Multiclass Acc: 0.4243\n",
            "Val Precision@Recall: 0.8993\n",
            "\n",
            "Epoch 31/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7271, Val Loss: 1.6845\n",
            "Train Binary Acc: 0.8702, Val Binary Acc: 0.8939\n",
            "Train Multiclass Acc: 0.4015, Val Multiclass Acc: 0.4298\n",
            "Val Precision@Recall: 0.8854\n",
            "\n",
            "Epoch 32/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7305, Val Loss: 1.6829\n",
            "Train Binary Acc: 0.8630, Val Binary Acc: 0.8988\n",
            "Train Multiclass Acc: 0.3962, Val Multiclass Acc: 0.4249\n",
            "Val Precision@Recall: 0.8879\n",
            "\n",
            "Epoch 33/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7315, Val Loss: 1.6807\n",
            "Train Binary Acc: 0.8633, Val Binary Acc: 0.8994\n",
            "Train Multiclass Acc: 0.3959, Val Multiclass Acc: 0.4310\n",
            "Val Precision@Recall: 0.8969\n",
            "\n",
            "Epoch 34/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 33: Updated optimal threshold to 0.4933 (F1: 0.9249, Precision: 0.9009)\n",
            "Train Loss: 1.7291, Val Loss: 1.6686\n",
            "Train Binary Acc: 0.8707, Val Binary Acc: 0.9001\n",
            "Train Multiclass Acc: 0.3988, Val Multiclass Acc: 0.4378\n",
            "Val Precision@Recall: 0.9009\n",
            "\n",
            "Epoch 35/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7186, Val Loss: 1.6795\n",
            "Train Binary Acc: 0.8666, Val Binary Acc: 0.8976\n",
            "Train Multiclass Acc: 0.4078, Val Multiclass Acc: 0.4341\n",
            "Val Precision@Recall: 0.8968\n",
            "\n",
            "Epoch 36/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7233, Val Loss: 1.6776\n",
            "Train Binary Acc: 0.8681, Val Binary Acc: 0.9001\n",
            "Train Multiclass Acc: 0.4069, Val Multiclass Acc: 0.4359\n",
            "Val Precision@Recall: 0.8984\n",
            "\n",
            "Epoch 37/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7233, Val Loss: 1.6711\n",
            "Train Binary Acc: 0.8690, Val Binary Acc: 0.8884\n",
            "Train Multiclass Acc: 0.4003, Val Multiclass Acc: 0.4421\n",
            "Val Precision@Recall: 0.8927\n",
            "\n",
            "Epoch 38/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7138, Val Loss: 1.6622\n",
            "Train Binary Acc: 0.8775, Val Binary Acc: 0.8970\n",
            "Train Multiclass Acc: 0.4155, Val Multiclass Acc: 0.4513\n",
            "Val Precision@Recall: 0.8968\n",
            "\n",
            "Epoch 39/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7130, Val Loss: 1.6684\n",
            "Train Binary Acc: 0.8770, Val Binary Acc: 0.8994\n",
            "Train Multiclass Acc: 0.4118, Val Multiclass Acc: 0.4414\n",
            "Val Precision@Recall: 0.8935\n",
            "\n",
            "Epoch 40/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7187, Val Loss: 1.6694\n",
            "Train Binary Acc: 0.8747, Val Binary Acc: 0.8994\n",
            "Train Multiclass Acc: 0.4133, Val Multiclass Acc: 0.4359\n",
            "Val Precision@Recall: 0.8910\n",
            "\n",
            "Epoch 41/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7172, Val Loss: 1.6732\n",
            "Train Binary Acc: 0.8727, Val Binary Acc: 0.8933\n",
            "Train Multiclass Acc: 0.4112, Val Multiclass Acc: 0.4378\n",
            "Val Precision@Recall: 0.8929\n",
            "\n",
            "Epoch 42/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.7048, Val Loss: 1.6573\n",
            "Train Binary Acc: 0.8816, Val Binary Acc: 0.9025\n",
            "Train Multiclass Acc: 0.4212, Val Multiclass Acc: 0.4506\n",
            "Val Precision@Recall: 0.9093\n",
            "\n",
            "Epoch 43/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 42: Updated optimal threshold to 0.5064 (F1: 0.9329, Precision: 0.9162)\n",
            "Train Loss: 1.6855, Val Loss: 1.6534\n",
            "Train Binary Acc: 0.8801, Val Binary Acc: 0.9129\n",
            "Train Multiclass Acc: 0.4408, Val Multiclass Acc: 0.4543\n",
            "Val Precision@Recall: 0.9162\n",
            "\n",
            "Epoch 44/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6934, Val Loss: 1.6558\n",
            "Train Binary Acc: 0.8807, Val Binary Acc: 0.9080\n",
            "Train Multiclass Acc: 0.4345, Val Multiclass Acc: 0.4562\n",
            "Val Precision@Recall: 0.9119\n",
            "\n",
            "Epoch 45/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6902, Val Loss: 1.6530\n",
            "Train Binary Acc: 0.8811, Val Binary Acc: 0.9099\n",
            "Train Multiclass Acc: 0.4359, Val Multiclass Acc: 0.4580\n",
            "Val Precision@Recall: 0.9076\n",
            "\n",
            "Epoch 46/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6850, Val Loss: 1.6464\n",
            "Train Binary Acc: 0.8819, Val Binary Acc: 0.9062\n",
            "Train Multiclass Acc: 0.4413, Val Multiclass Acc: 0.4598\n",
            "Val Precision@Recall: 0.9079\n",
            "\n",
            "Epoch 47/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6836, Val Loss: 1.6453\n",
            "Train Binary Acc: 0.8888, Val Binary Acc: 0.9129\n",
            "Train Multiclass Acc: 0.4443, Val Multiclass Acc: 0.4641\n",
            "Val Precision@Recall: 0.9137\n",
            "\n",
            "Epoch 48/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6844, Val Loss: 1.6472\n",
            "Train Binary Acc: 0.8859, Val Binary Acc: 0.9105\n",
            "Train Multiclass Acc: 0.4367, Val Multiclass Acc: 0.4623\n",
            "Val Precision@Recall: 0.9119\n",
            "\n",
            "Epoch 49/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 48: Updated optimal threshold to 0.5310 (F1: 0.9329, Precision: 0.9218)\n",
            "Train Loss: 1.6824, Val Loss: 1.6399\n",
            "Train Binary Acc: 0.8851, Val Binary Acc: 0.9099\n",
            "Train Multiclass Acc: 0.4439, Val Multiclass Acc: 0.4697\n",
            "Val Precision@Recall: 0.9119\n",
            "\n",
            "Epoch 50/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6817, Val Loss: 1.6447\n",
            "Train Binary Acc: 0.8828, Val Binary Acc: 0.9086\n",
            "Train Multiclass Acc: 0.4393, Val Multiclass Acc: 0.4635\n",
            "Val Precision@Recall: 0.9145\n",
            "\n",
            "Epoch 51/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6860, Val Loss: 1.6433\n",
            "Train Binary Acc: 0.8814, Val Binary Acc: 0.9142\n",
            "Train Multiclass Acc: 0.4376, Val Multiclass Acc: 0.4684\n",
            "Val Precision@Recall: 0.9171\n",
            "\n",
            "Epoch 52/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 51: Updated optimal threshold to 0.4711 (F1: 0.9335, Precision: 0.9025)\n",
            "Train Loss: 1.6746, Val Loss: 1.6384\n",
            "Train Binary Acc: 0.8865, Val Binary Acc: 0.9105\n",
            "Train Multiclass Acc: 0.4480, Val Multiclass Acc: 0.4666\n",
            "Val Precision@Recall: 0.9121\n",
            "\n",
            "Epoch 53/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6762, Val Loss: 1.6400\n",
            "Train Binary Acc: 0.8928, Val Binary Acc: 0.9086\n",
            "Train Multiclass Acc: 0.4500, Val Multiclass Acc: 0.4678\n",
            "Val Precision@Recall: 0.9086\n",
            "\n",
            "Epoch 54/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6698, Val Loss: 1.6388\n",
            "Train Binary Acc: 0.8903, Val Binary Acc: 0.9142\n",
            "Train Multiclass Acc: 0.4544, Val Multiclass Acc: 0.4727\n",
            "Val Precision@Recall: 0.9137\n",
            "\n",
            "Epoch 55/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 54: Updated optimal threshold to 0.4617 (F1: 0.9339, Precision: 0.9164)\n",
            "Train Loss: 1.6704, Val Loss: 1.6446\n",
            "Train Binary Acc: 0.8905, Val Binary Acc: 0.9111\n",
            "Train Multiclass Acc: 0.4589, Val Multiclass Acc: 0.4617\n",
            "Val Precision@Recall: 0.9171\n",
            "\n",
            "Epoch 56/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6728, Val Loss: 1.6347\n",
            "Train Binary Acc: 0.8949, Val Binary Acc: 0.9086\n",
            "Train Multiclass Acc: 0.4538, Val Multiclass Acc: 0.4733\n",
            "Val Precision@Recall: 0.9232\n",
            "\n",
            "Epoch 57/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6730, Val Loss: 1.6369\n",
            "Train Binary Acc: 0.8954, Val Binary Acc: 0.9135\n",
            "Train Multiclass Acc: 0.4532, Val Multiclass Acc: 0.4727\n",
            "Val Precision@Recall: 0.9162\n",
            "\n",
            "Epoch 58/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 57: Updated optimal threshold to 0.5113 (F1: 0.9359, Precision: 0.9239)\n",
            "Train Loss: 1.6731, Val Loss: 1.6428\n",
            "Train Binary Acc: 0.8893, Val Binary Acc: 0.9160\n",
            "Train Multiclass Acc: 0.4523, Val Multiclass Acc: 0.4684\n",
            "Val Precision@Recall: 0.9188\n",
            "\n",
            "Epoch 59/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6704, Val Loss: 1.6391\n",
            "Train Binary Acc: 0.8877, Val Binary Acc: 0.9135\n",
            "Train Multiclass Acc: 0.4575, Val Multiclass Acc: 0.4697\n",
            "Val Precision@Recall: 0.9153\n",
            "\n",
            "Epoch 60/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6719, Val Loss: 1.6396\n",
            "Train Binary Acc: 0.8893, Val Binary Acc: 0.9117\n",
            "Train Multiclass Acc: 0.4560, Val Multiclass Acc: 0.4684\n",
            "Val Precision@Recall: 0.9179\n",
            "\n",
            "Epoch 61/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6729, Val Loss: 1.6368\n",
            "Train Binary Acc: 0.8940, Val Binary Acc: 0.9062\n",
            "Train Multiclass Acc: 0.4531, Val Multiclass Acc: 0.4733\n",
            "Val Precision@Recall: 0.9179\n",
            "\n",
            "Epoch 62/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6646, Val Loss: 1.6359\n",
            "Train Binary Acc: 0.8933, Val Binary Acc: 0.9117\n",
            "Train Multiclass Acc: 0.4557, Val Multiclass Acc: 0.4733\n",
            "Val Precision@Recall: 0.9136\n",
            "\n",
            "Epoch 63/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6668, Val Loss: 1.6349\n",
            "Train Binary Acc: 0.8906, Val Binary Acc: 0.9142\n",
            "Train Multiclass Acc: 0.4610, Val Multiclass Acc: 0.4709\n",
            "Val Precision@Recall: 0.9188\n",
            "\n",
            "Epoch 64/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6703, Val Loss: 1.6385\n",
            "Train Binary Acc: 0.8897, Val Binary Acc: 0.9135\n",
            "Train Multiclass Acc: 0.4514, Val Multiclass Acc: 0.4666\n",
            "Val Precision@Recall: 0.9188\n",
            "\n",
            "Epoch 65/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6697, Val Loss: 1.6347\n",
            "Train Binary Acc: 0.8857, Val Binary Acc: 0.9117\n",
            "Train Multiclass Acc: 0.4541, Val Multiclass Acc: 0.4739\n",
            "Val Precision@Recall: 0.9179\n",
            "\n",
            "Epoch 66/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6663, Val Loss: 1.6351\n",
            "Train Binary Acc: 0.8905, Val Binary Acc: 0.9135\n",
            "Train Multiclass Acc: 0.4604, Val Multiclass Acc: 0.4684\n",
            "Val Precision@Recall: 0.9189\n",
            "\n",
            "Epoch 67/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6723, Val Loss: 1.6344\n",
            "Train Binary Acc: 0.8900, Val Binary Acc: 0.9148\n",
            "Train Multiclass Acc: 0.4526, Val Multiclass Acc: 0.4727\n",
            "Val Precision@Recall: 0.9197\n",
            "\n",
            "Epoch 68/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6683, Val Loss: 1.6381\n",
            "Train Binary Acc: 0.8920, Val Binary Acc: 0.9105\n",
            "Train Multiclass Acc: 0.4537, Val Multiclass Acc: 0.4684\n",
            "Val Precision@Recall: 0.9129\n",
            "\n",
            "Epoch 69/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6682, Val Loss: 1.6417\n",
            "Train Binary Acc: 0.8936, Val Binary Acc: 0.9154\n",
            "Train Multiclass Acc: 0.4584, Val Multiclass Acc: 0.4666\n",
            "Val Precision@Recall: 0.9188\n",
            "\n",
            "Epoch 70/80\n",
            "--------------------------------------------------\n",
            "\n",
            "Epoch 69: Updated optimal threshold to 0.5414 (F1: 0.9364, Precision: 0.9314)\n",
            "Train Loss: 1.6686, Val Loss: 1.6334\n",
            "Train Binary Acc: 0.8920, Val Binary Acc: 0.9148\n",
            "Train Multiclass Acc: 0.4555, Val Multiclass Acc: 0.4746\n",
            "Val Precision@Recall: 0.9197\n",
            "\n",
            "Epoch 71/80\n",
            "--------------------------------------------------\n",
            "Train Loss: 1.6688, Val Loss: 1.6348\n",
            "Train Binary Acc: 0.8963, Val Binary Acc: 0.9123\n",
            "Train Multiclass Acc: 0.4609, Val Multiclass Acc: 0.4752\n",
            "Val Precision@Recall: 0.9145\n",
            "Early stopping triggered after 15 epochs without improvement\n",
            "Training complete. Best validation precision@recall: 0.9232\n",
            "Optimal threshold: 0.5414\n",
            "Evaluating model...\n",
            "\n",
            "Binary Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      0.86      0.88       607\n",
            "         1.0       0.92      0.94      0.93      1024\n",
            "\n",
            "    accuracy                           0.91      1631\n",
            "   macro avg       0.91      0.90      0.91      1631\n",
            "weighted avg       0.91      0.91      0.91      1631\n",
            "\n",
            "\n",
            "Binary Confusion Matrix:\n",
            "[[522  85]\n",
            " [ 58 966]]\n",
            "\n",
            "Multiclass Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.63      0.65       128\n",
            "           1       0.50      0.55      0.52       128\n",
            "           2       0.00      0.00      0.00        32\n",
            "           3       0.33      0.12      0.17       128\n",
            "           4       0.36      0.41      0.38       128\n",
            "           5       0.00      0.00      0.00        32\n",
            "           6       0.61      0.42      0.50       128\n",
            "           7       0.48      0.57      0.52       128\n",
            "           8       0.00      0.00      0.00        32\n",
            "           9       0.44      0.66      0.53       128\n",
            "          10       0.21      0.23      0.22       128\n",
            "          11       0.00      0.00      0.00        32\n",
            "          12       0.55      0.77      0.64        96\n",
            "          13       0.00      0.00      0.00        32\n",
            "          14       0.57      0.88      0.69       128\n",
            "          15       0.71      0.46      0.56        96\n",
            "          16       0.53      0.63      0.58        95\n",
            "          17       0.27      0.78      0.40        32\n",
            "\n",
            "    accuracy                           0.48      1631\n",
            "   macro avg       0.34      0.40      0.35      1631\n",
            "weighted avg       0.44      0.48      0.44      1631\n",
            "\n",
            "Generating predictions...\n",
            "Predictions saved to pytorch_precision_optimized_predictions.csv\n",
            "PyTorch model saved to pytorch_precision_optimized_bfrb_model with metadata\n",
            "PyTorch BFRB detection pipeline complete with precision optimization\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import cuML for GPU acceleration\n",
        "try:\n",
        "    import cudf\n",
        "    import cuml\n",
        "    from cuml.preprocessing import StandardScaler as cuMLStandardScaler\n",
        "    from cuml.model_selection import train_test_split as cuml_train_test_split\n",
        "    CUML_AVAILABLE = True\n",
        "    print(\"cuML available - GPU acceleration enabled\")\n",
        "except ImportError:\n",
        "    CUML_AVAILABLE = False\n",
        "    print(\"cuML not available - using CPU preprocessing\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Constants for the model\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "NUM_IMU_FEATURES = 7\n",
        "NUM_THERMOPILE = 5\n",
        "NUM_TOF_SENSORS = 5\n",
        "TOF_PIXELS_PER_SENSOR = 64\n",
        "NUM_DEMOGRAPHIC_FEATURES = 7\n",
        "\n",
        "# Global variables for class weights\n",
        "BINARY_CLASS_WEIGHTS = None\n",
        "MULTICLASS_CLASS_WEIGHTS = None\n",
        "\n",
        "class AsymmetricFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha_pos=0.25, alpha_neg=0.75, gamma_pos=1.0, gamma_neg=4.0):\n",
        "        \"\"\"\n",
        "        Asymmetric Focal Loss for binary classification to improve precision\n",
        "\n",
        "        Args:\n",
        "            alpha_pos: Weight for positive samples (lower to reduce false positives)\n",
        "            alpha_neg: Weight for negative samples (higher to emphasize true negatives)\n",
        "            gamma_pos: Focusing parameter for positive samples (lower focusing)\n",
        "            gamma_neg: Focusing parameter for negative samples (higher focusing on hard negatives)\n",
        "        \"\"\"\n",
        "        super(AsymmetricFocalLoss, self).__init__()\n",
        "        self.alpha_pos = alpha_pos\n",
        "        self.alpha_neg = alpha_neg\n",
        "        self.gamma_pos = gamma_pos\n",
        "        self.gamma_neg = gamma_neg\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = y_true.float()\n",
        "        y_pred = torch.clamp(y_pred, min=1e-8, max=1.0 - 1e-8)\n",
        "\n",
        "        # Calculate asymmetric focal loss components\n",
        "        # For positive samples (y_true = 1)\n",
        "        pos_loss = -self.alpha_pos * torch.pow(1 - y_pred, self.gamma_pos) * torch.log(y_pred)\n",
        "\n",
        "        # For negative samples (y_true = 0)\n",
        "        neg_loss = -self.alpha_neg * torch.pow(y_pred, self.gamma_neg) * torch.log(1 - y_pred)\n",
        "\n",
        "        # Apply based on true labels\n",
        "        loss = torch.where(y_true == 1, pos_loss, neg_loss)\n",
        "\n",
        "        return torch.mean(loss)\n",
        "\n",
        "class SparseCategoricalFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=2.0, gamma=2.5, num_classes=18):\n",
        "        super(SparseCategoricalFocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_true = y_true.long()\n",
        "        y_pred = F.softmax(y_pred, dim=1)\n",
        "        y_pred = torch.clamp(y_pred, min=1e-8, max=1.0 - 1e-8)\n",
        "\n",
        "        # Convert sparse labels to one-hot\n",
        "        y_true_one_hot = F.one_hot(y_true, num_classes=self.num_classes).float()\n",
        "\n",
        "        # Calculate focal loss with class-aware weighting\n",
        "        ce_loss = -torch.sum(y_true_one_hot * torch.log(y_pred), dim=1)\n",
        "        p_t = torch.sum(y_true_one_hot * y_pred, dim=1)\n",
        "        focal_weight = self.alpha * torch.pow(1 - p_t, self.gamma)\n",
        "\n",
        "        focal_loss = focal_weight * ce_loss\n",
        "        return torch.mean(focal_loss)\n",
        "\n",
        "# Custom Metrics\n",
        "class WeightedF1Score:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tp = 0\n",
        "        self.fp = 0\n",
        "        self.fn = 0\n",
        "\n",
        "    def update(self, y_pred, y_true):\n",
        "        y_pred_binary = (y_pred >= 0.5).float()\n",
        "        self.tp += torch.sum((y_pred_binary == 1) & (y_true == 1)).item()\n",
        "        self.fp += torch.sum((y_pred_binary == 1) & (y_true == 0)).item()\n",
        "        self.fn += torch.sum((y_pred_binary == 0) & (y_true == 1)).item()\n",
        "\n",
        "    def compute(self):\n",
        "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
        "        recall = self.tp / (self.tp + self.fn + 1e-8)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "        return f1\n",
        "\n",
        "class MacroF1Score:\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tp = torch.zeros(self.num_classes)\n",
        "        self.fp = torch.zeros(self.num_classes)\n",
        "        self.fn = torch.zeros(self.num_classes)\n",
        "\n",
        "    def update(self, y_pred, y_true):\n",
        "        y_pred_classes = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "        for i in range(self.num_classes):\n",
        "            self.tp[i] += torch.sum((y_pred_classes == i) & (y_true == i)).item()\n",
        "            self.fp[i] += torch.sum((y_pred_classes == i) & (y_true != i)).item()\n",
        "            self.fn[i] += torch.sum((y_pred_classes != i) & (y_true == i)).item()\n",
        "\n",
        "    def compute(self):\n",
        "        precision = self.tp / (self.tp + self.fp + 1e-8)\n",
        "        recall = self.tp / (self.tp + self.fn + 1e-8)\n",
        "        f1_per_class = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "        return torch.mean(f1_per_class).item()\n",
        "\n",
        "class PrecisionAtRecall:\n",
        "    def __init__(self, target_recall=0.95):\n",
        "        self.target_recall = target_recall\n",
        "        self.predictions = []\n",
        "        self.targets = []\n",
        "\n",
        "    def reset(self):\n",
        "        self.predictions = []\n",
        "        self.targets = []\n",
        "\n",
        "    def update(self, y_pred, y_true):\n",
        "        self.predictions.extend(y_pred.cpu().detach().numpy())\n",
        "        self.targets.extend(y_true.cpu().detach().numpy())\n",
        "\n",
        "    def compute(self):\n",
        "        if len(self.predictions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision, recall, thresholds = precision_recall_curve(\n",
        "            self.targets, self.predictions\n",
        "        )\n",
        "\n",
        "        # Find precision at target recall\n",
        "        valid_indices = recall >= self.target_recall\n",
        "        if np.any(valid_indices):\n",
        "            return np.max(precision[valid_indices])\n",
        "        return 0.0\n",
        "\n",
        "# Time Series Data Augmentation Functions\n",
        "def time_series_augmentation(data, augment_prob=0.5):\n",
        "    \"\"\"Apply various time series augmentation techniques\"\"\"\n",
        "    if np.random.random() < augment_prob:\n",
        "        augmentation_type = np.random.choice(['jitter', 'scale', 'time_warp', 'magnitude_warp'])\n",
        "\n",
        "        if augmentation_type == 'jitter':\n",
        "            # Add random noise\n",
        "            noise = np.random.normal(0, 0.03, data.shape)\n",
        "            return data + noise\n",
        "\n",
        "        elif augmentation_type == 'scale':\n",
        "            # Scale the magnitude\n",
        "            scale_factor = np.random.uniform(0.8, 1.2)\n",
        "            return data * scale_factor\n",
        "\n",
        "        elif augmentation_type == 'time_warp':\n",
        "            # Time warping by random sampling\n",
        "            seq_len = data.shape[0]\n",
        "            indices = np.sort(np.random.choice(seq_len, int(seq_len * 0.9), replace=False))\n",
        "            warped_data = np.zeros_like(data)\n",
        "            warped_indices = np.linspace(0, seq_len-1, len(indices)).astype(int)\n",
        "            warped_data[warped_indices] = data[indices]\n",
        "            return warped_data\n",
        "\n",
        "        elif augmentation_type == 'magnitude_warp':\n",
        "            # Smooth magnitude warping\n",
        "            warp = np.random.normal(1.0, 0.1, data.shape[1])\n",
        "            return data * warp\n",
        "\n",
        "    return data\n",
        "\n",
        "def load_and_preprocess_data(train_file, train_demographics_file, test_file=None, test_demographics_file=None):\n",
        "    \"\"\"Enhanced data loading with proper class balance analysis\"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    if CUML_AVAILABLE:\n",
        "        train_df = cudf.read_csv(train_file)\n",
        "        train_demo_df = cudf.read_csv(train_demographics_file)\n",
        "    else:\n",
        "        train_df = pd.read_csv(train_file)\n",
        "        train_demo_df = pd.read_csv(train_demographics_file)\n",
        "\n",
        "    print(\"Processing training sequences...\")\n",
        "    train_sequences = process_sequences(train_df, train_demo_df)\n",
        "\n",
        "    print(\"Preparing training data for model...\")\n",
        "    X_train, y_train = prepare_model_data(train_sequences)\n",
        "\n",
        "    # Analyze class distribution with proper handling\n",
        "    binary_counts = np.bincount(y_train['binary_output'])\n",
        "    multiclass_counts = np.bincount(y_train['multiclass_output'])\n",
        "\n",
        "    print(f\"Binary class distribution: {binary_counts}\")\n",
        "    print(f\"Multiclass class distribution: {multiclass_counts}\")\n",
        "    print(f\"Number of unique multiclass labels: {len(multiclass_counts)}\")\n",
        "    print(f\"Class imbalance ratio: {binary_counts[0]/binary_counts[1] if len(binary_counts) > 1 else 'No positive samples'}\")\n",
        "\n",
        "    # Calculate improved class weights for imbalanced data\n",
        "    global BINARY_CLASS_WEIGHTS, MULTICLASS_CLASS_WEIGHTS\n",
        "\n",
        "    if len(binary_counts) > 1:\n",
        "        # More aggressive rebalancing for binary classification\n",
        "        pos_weight = binary_counts[0] / binary_counts[1]\n",
        "        BINARY_CLASS_WEIGHTS = np.array([1.0, pos_weight * 1.5])  # Extra weight for positive class\n",
        "        print(f\"Enhanced binary class weights: {BINARY_CLASS_WEIGHTS}\")\n",
        "    else:\n",
        "        BINARY_CLASS_WEIGHTS = np.array([1.0, 1.0])\n",
        "\n",
        "    # Enhanced multiclass class weights with stronger minority class support\n",
        "    unique_classes = np.unique(y_train['multiclass_output'])\n",
        "    n_samples = len(y_train['multiclass_output'])\n",
        "    n_classes = len(unique_classes)\n",
        "\n",
        "    # Calculate balanced weights with extra boost for very rare classes\n",
        "    class_weights = []\n",
        "    for class_id in unique_classes:\n",
        "        class_count = np.sum(y_train['multiclass_output'] == class_id)\n",
        "        # Standard balanced weight with additional minority boost\n",
        "        base_weight = n_samples / (n_classes * class_count)\n",
        "        # Extra boost for classes with very few samples (< 5% of average)\n",
        "        avg_samples = n_samples / n_classes\n",
        "        if class_count < 0.05 * avg_samples:\n",
        "            minority_boost = 2.0\n",
        "        elif class_count < 0.2 * avg_samples:\n",
        "            minority_boost = 1.5\n",
        "        else:\n",
        "            minority_boost = 1.0\n",
        "\n",
        "        final_weight = base_weight * minority_boost\n",
        "        class_weights.append(final_weight)\n",
        "\n",
        "    MULTICLASS_CLASS_WEIGHTS = np.array(class_weights)\n",
        "    print(f\"Enhanced multiclass class weights shape: {MULTICLASS_CLASS_WEIGHTS.shape}\")\n",
        "    print(f\"Multiclass weights range: {MULTICLASS_CLASS_WEIGHTS.min():.3f} - {MULTICLASS_CLASS_WEIGHTS.max():.3f}\")\n",
        "\n",
        "    # Create train/validation split with stratification on both tasks\n",
        "    # Create combined stratification key\n",
        "    stratify_key = [f\"{binary}_{multi}\" for binary, multi in zip(y_train['binary_output'], y_train['multiclass_output'])]\n",
        "\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        range(len(X_train['imu_input'])),\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=stratify_key  # Stratify on both binary and multiclass\n",
        "    )\n",
        "\n",
        "    X_train_split = {key: value[train_indices] for key, value in X_train.items()}\n",
        "    X_val = {key: value[val_indices] for key, value in X_train.items()}\n",
        "    y_train_split = {key: value[train_indices] for key, value in y_train.items()}\n",
        "    y_val = {key: value[val_indices] for key, value in y_train.items()}\n",
        "\n",
        "    # Verify class distribution in splits\n",
        "    val_binary_counts = np.bincount(y_val['binary_output'])\n",
        "    print(f\"Validation binary distribution: {val_binary_counts}\")\n",
        "\n",
        "    data_dict = {\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'X_val': X_val,\n",
        "        'y_val': y_val\n",
        "    }\n",
        "\n",
        "    # Load test data if provided\n",
        "    if test_file and test_demographics_file:\n",
        "        print(\"Loading test data...\")\n",
        "        if CUML_AVAILABLE:\n",
        "            test_df = cudf.read_csv(test_file)\n",
        "            test_demo_df = cudf.read_csv(test_demographics_file)\n",
        "        else:\n",
        "            test_df = pd.read_csv(test_file)\n",
        "            test_demo_df = pd.read_csv(test_demographics_file)\n",
        "\n",
        "        print(\"Processing test sequences...\")\n",
        "        test_sequences = process_sequences(test_df, test_demo_df, is_train=False)\n",
        "\n",
        "        print(\"Preparing test data for model...\")\n",
        "        X_test, sequence_ids = prepare_model_data(test_sequences, is_train=False)\n",
        "\n",
        "        data_dict['X_test'] = X_test\n",
        "        data_dict['sequence_ids'] = sequence_ids\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def process_sequences(df, demo_df, is_train=True):\n",
        "    \"\"\"sequence processing with better error handling\"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    if CUML_AVAILABLE:\n",
        "        sequence_ids = df['sequence_id'].unique().values_host\n",
        "    else:\n",
        "        sequence_ids = df['sequence_id'].unique()\n",
        "\n",
        "    for seq_id in sequence_ids:\n",
        "        seq_data = df[df['sequence_id'] == seq_id]\n",
        "\n",
        "        if len(seq_data) == 0:\n",
        "            print(f\"Warning: Empty sequence data for sequence ID {seq_id}\")\n",
        "            continue\n",
        "\n",
        "        if CUML_AVAILABLE:\n",
        "            seq_data = seq_data.to_pandas()\n",
        "            demo_df_pandas = demo_df.to_pandas()\n",
        "        else:\n",
        "            demo_df_pandas = demo_df\n",
        "\n",
        "        subject_id = seq_data['subject'].iloc[0]\n",
        "        subject_demo = demo_df_pandas[demo_df_pandas['subject'] == subject_id]\n",
        "\n",
        "        if len(subject_demo) == 0:\n",
        "            print(f\"Warning: No demographic data found for subject {subject_id}\")\n",
        "            # Use default demographic values\n",
        "            demo_features = [0.0, 25.0, 0.0, 1.0, 170.0, 52.0, 28.0]\n",
        "        else:\n",
        "            demo_features = [\n",
        "                subject_demo['adult_child'].fillna(0).iloc[0],\n",
        "                subject_demo['age'].fillna(25).iloc[0],\n",
        "                subject_demo['sex'].fillna(0).iloc[0],\n",
        "                subject_demo['handedness'].fillna(1).iloc[0],\n",
        "                subject_demo['height_cm'].fillna(170).iloc[0],\n",
        "                subject_demo['shoulder_to_wrist_cm'].fillna(52).iloc[0],\n",
        "                subject_demo['elbow_to_wrist_cm'].fillna(28).iloc[0]\n",
        "            ]\n",
        "\n",
        "        # Extract sensor columns\n",
        "        imu_cols = [col for col in seq_data.columns if col.startswith('acc_') or col.startswith('rot_')]\n",
        "        thm_cols = [col for col in seq_data.columns if col.startswith('thm_')]\n",
        "        tof_cols = [col for col in seq_data.columns if col.startswith('tof_')]\n",
        "\n",
        "        # Handle sequence length with padding/truncation\n",
        "        if len(seq_data) < MAX_SEQUENCE_LENGTH:\n",
        "            padding_needed = MAX_SEQUENCE_LENGTH - len(seq_data)\n",
        "            last_row = seq_data.iloc[-1:].copy()\n",
        "            for _ in range(padding_needed):\n",
        "                seq_data = pd.concat([seq_data, last_row])\n",
        "        elif len(seq_data) > MAX_SEQUENCE_LENGTH:\n",
        "            seq_data = seq_data.iloc[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        # Extract and process sensor data\n",
        "        imu_data = seq_data[imu_cols].fillna(0).values\n",
        "        thm_data = seq_data[thm_cols].fillna(0).values\n",
        "        tof_data = seq_data[tof_cols].fillna(0).values\n",
        "\n",
        "        # Apply data augmentation for training data\n",
        "        if is_train:\n",
        "            imu_data = time_series_augmentation(imu_data)\n",
        "            thm_data = time_series_augmentation(thm_data)\n",
        "\n",
        "        # Create ToF mask and normalize\n",
        "        tof_mask = (tof_data != -1).astype(np.float32)\n",
        "        tof_data = np.where(tof_data == -1, 0, tof_data)\n",
        "\n",
        "        sequence = {\n",
        "            'sequence_id': seq_id,\n",
        "            'imu_data': imu_data,\n",
        "            'thm_data': thm_data,\n",
        "            'tof_data': tof_data,\n",
        "            'tof_mask': tof_mask,\n",
        "            'demographic': demo_features,\n",
        "        }\n",
        "\n",
        "        if is_train:\n",
        "            sequence_type = seq_data['sequence_type'].iloc[0]\n",
        "            gesture = seq_data['gesture'].iloc[0]\n",
        "            binary_target = 1 if sequence_type == 'Target' else 0\n",
        "\n",
        "            sequence['binary_target'] = binary_target\n",
        "            sequence['gesture'] = gesture\n",
        "\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def prepare_model_data(sequences, is_train=True):\n",
        "    \"\"\"data preparation with improved normalization\"\"\"\n",
        "    imu_data = []\n",
        "    thm_data = []\n",
        "    tof_data = []\n",
        "    tof_mask = []\n",
        "    demo_data = []\n",
        "    binary_targets = []\n",
        "    gesture_targets = []\n",
        "    sequence_ids = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        imu_data.append(seq['imu_data'])\n",
        "        thm_data.append(seq['thm_data'])\n",
        "        tof_data.append(seq['tof_data'])\n",
        "        tof_mask.append(seq['tof_mask'])\n",
        "        demo_data.append(seq['demographic'])\n",
        "        sequence_ids.append(seq['sequence_id'])\n",
        "\n",
        "        if is_train:\n",
        "            binary_targets.append(seq['binary_target'])\n",
        "            gesture_targets.append(seq['gesture'])\n",
        "\n",
        "    # Convert CuPy arrays to NumPy\n",
        "    def cupy_to_numpy(data):\n",
        "        if hasattr(data, 'get'):\n",
        "            return data.get()\n",
        "        elif isinstance(data, list):\n",
        "            return [cupy_to_numpy(item) for item in data]\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "    imu_data = [cupy_to_numpy(item) for item in imu_data]\n",
        "    thm_data = [cupy_to_numpy(item) for item in thm_data]\n",
        "    tof_data = [cupy_to_numpy(item) for item in tof_data]\n",
        "    tof_mask = [cupy_to_numpy(item) for item in tof_mask]\n",
        "    demo_data = [cupy_to_numpy(item) for item in demo_data]\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    imu_data = np.array(imu_data)\n",
        "    thm_data = np.array(thm_data)\n",
        "    tof_data = np.array(tof_data)\n",
        "    tof_mask = np.array(tof_mask)\n",
        "    demo_data = np.array(demo_data)\n",
        "\n",
        "    # normalization\n",
        "    from sklearn.preprocessing import StandardScaler as SKStandardScaler, RobustScaler\n",
        "\n",
        "    # Use RobustScaler for better outlier handling\n",
        "    imu_scaler = RobustScaler()\n",
        "    thm_scaler = RobustScaler()\n",
        "    demo_scaler = SKStandardScaler()\n",
        "\n",
        "    # Normalize sensor data\n",
        "    imu_shape = imu_data.shape\n",
        "    thm_shape = thm_data.shape\n",
        "\n",
        "    imu_flat = imu_data.reshape(-1, imu_shape[2])\n",
        "    thm_flat = thm_data.reshape(-1, thm_shape[2])\n",
        "\n",
        "    # Fit and transform with outlier-robust scaling\n",
        "    imu_flat = imu_scaler.fit_transform(imu_flat)\n",
        "    thm_flat = thm_scaler.fit_transform(thm_flat)\n",
        "    demo_data = demo_scaler.fit_transform(demo_data)\n",
        "\n",
        "    imu_data = imu_flat.reshape(imu_shape)\n",
        "    thm_data = thm_flat.reshape(thm_shape)\n",
        "\n",
        "    # Improved ToF normalization\n",
        "    tof_data = np.where(tof_mask == 1, tof_data / 254.0, 0)\n",
        "\n",
        "    X = {\n",
        "        'imu_input': imu_data,\n",
        "        'thm_input': thm_data,\n",
        "        'tof_input': tof_data,\n",
        "        'tof_mask': tof_mask,\n",
        "        'demo_input': demo_data\n",
        "    }\n",
        "\n",
        "    if is_train:\n",
        "        binary_targets = cupy_to_numpy(binary_targets)\n",
        "        gesture_targets = cupy_to_numpy(gesture_targets)\n",
        "\n",
        "        binary_targets = np.array(binary_targets)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "        gesture_targets = label_encoder.fit_transform(gesture_targets)\n",
        "        gesture_targets = np.array(gesture_targets)\n",
        "\n",
        "        y = {\n",
        "            'binary_output': binary_targets,\n",
        "            'multiclass_output': gesture_targets\n",
        "        }\n",
        "\n",
        "        return X, y\n",
        "    else:\n",
        "        return X, sequence_ids\n",
        "\n",
        "class MultimodalBFRBCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        num_imu_features=NUM_IMU_FEATURES,\n",
        "        num_thermopile=NUM_THERMOPILE,\n",
        "        num_tof_sensors=NUM_TOF_SENSORS,\n",
        "        tof_pixels=TOF_PIXELS_PER_SENSOR,\n",
        "        num_demographic=NUM_DEMOGRAPHIC_FEATURES,\n",
        "        num_gestures=18\n",
        "    ):\n",
        "        super(MultimodalBFRBCNN, self).__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_tof_sensors = num_tof_sensors\n",
        "\n",
        "        # IMU branch with residual connections\n",
        "        self.imu_conv1 = nn.Conv1d(num_imu_features, 64, kernel_size=5, padding=2)\n",
        "        self.imu_bn1 = nn.BatchNorm1d(64)\n",
        "        self.imu_dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        # Residual block\n",
        "        self.imu_residual_conv = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "        self.imu_residual_bn = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.imu_conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.imu_bn2 = nn.BatchNorm1d(128)\n",
        "        self.imu_pool = nn.MaxPool1d(2)\n",
        "        self.imu_dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.imu_conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.imu_bn3 = nn.BatchNorm1d(256)\n",
        "        self.imu_global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.imu_dropout3 = nn.Dropout(0.4)\n",
        "\n",
        "        # Thermopile branch\n",
        "        self.thm_conv1 = nn.Conv1d(num_thermopile, 32, kernel_size=3, padding=1)\n",
        "        self.thm_bn1 = nn.BatchNorm1d(32)\n",
        "        self.thm_dropout1 = nn.Dropout(0.2)\n",
        "\n",
        "        self.thm_conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.thm_bn2 = nn.BatchNorm1d(64)\n",
        "        self.thm_pool = nn.MaxPool1d(2)\n",
        "        self.thm_dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        self.thm_global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.thm_dropout3 = nn.Dropout(0.4)\n",
        "\n",
        "        # ToF branch (simplified 2D CNN approach)\n",
        "        self.tof_conv1 = nn.Conv2d(num_tof_sensors, 32, kernel_size=3, padding=1)\n",
        "        self.tof_bn1 = nn.BatchNorm2d(32)\n",
        "        self.tof_pool1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.tof_conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.tof_bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.tof_global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.tof_dropout = nn.Dropout(0.4)\n",
        "\n",
        "        # ToF attention mechanism\n",
        "        self.tof_attention_fc1 = nn.Linear(1, 64)\n",
        "        self.tof_attention_fc2 = nn.Linear(64, 64)\n",
        "\n",
        "        # Demographic branch\n",
        "        self.demo_fc1 = nn.Linear(num_demographic, 64)\n",
        "        self.demo_bn1 = nn.BatchNorm1d(64)\n",
        "        self.demo_dropout1 = nn.Dropout(0.3)\n",
        "\n",
        "        self.demo_fc2 = nn.Linear(64, 32)\n",
        "        self.demo_bn2 = nn.BatchNorm1d(32)\n",
        "        self.demo_dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "        # Calculate feature dimensions after concatenation\n",
        "        concat_dim = 256 + 64 + 64 + 32  # IMU + Thermopile + ToF + Demo\n",
        "\n",
        "        # Feature attention\n",
        "        self.feature_attention = nn.Linear(concat_dim, concat_dim)\n",
        "\n",
        "        # Shared layers\n",
        "        self.shared_fc1 = nn.Linear(concat_dim, 512)\n",
        "        self.shared_bn1 = nn.BatchNorm1d(512)\n",
        "        self.shared_dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "        self.shared_fc2 = nn.Linear(512, 256)\n",
        "        self.shared_bn2 = nn.BatchNorm1d(256)\n",
        "        self.shared_dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.shared_fc3 = nn.Linear(256, 128)\n",
        "        self.shared_bn3 = nn.BatchNorm1d(128)\n",
        "        self.shared_dropout3 = nn.Dropout(0.4)\n",
        "\n",
        "        # Output layers\n",
        "        self.binary_output = nn.Linear(128, 1)\n",
        "        self.multiclass_output = nn.Linear(128, num_gestures)\n",
        "\n",
        "    def forward(self, imu_input, thm_input, tof_input, tof_mask, demo_input):\n",
        "        batch_size = imu_input.size(0)\n",
        "\n",
        "        # IMU branch\n",
        "        x_imu = imu_input.transpose(1, 2)  # (batch, features, sequence)\n",
        "        x_imu = F.relu(self.imu_bn1(self.imu_conv1(x_imu)))\n",
        "        x_imu = self.imu_dropout1(x_imu)\n",
        "\n",
        "        # Residual connection\n",
        "        imu_residual = F.relu(self.imu_residual_bn(self.imu_residual_conv(x_imu)))\n",
        "        x_imu = x_imu + imu_residual\n",
        "\n",
        "        x_imu = F.relu(self.imu_bn2(self.imu_conv2(x_imu)))\n",
        "        x_imu = self.imu_pool(x_imu)\n",
        "        x_imu = self.imu_dropout2(x_imu)\n",
        "\n",
        "        x_imu = F.relu(self.imu_bn3(self.imu_conv3(x_imu)))\n",
        "        x_imu = self.imu_global_pool(x_imu)\n",
        "        x_imu = self.imu_dropout3(x_imu)\n",
        "        x_imu = x_imu.view(batch_size, -1)\n",
        "\n",
        "        # Thermopile branch\n",
        "        x_thm = thm_input.transpose(1, 2)  # (batch, features, sequence)\n",
        "        x_thm = F.relu(self.thm_bn1(self.thm_conv1(x_thm)))\n",
        "        x_thm = self.thm_dropout1(x_thm)\n",
        "\n",
        "        x_thm = F.relu(self.thm_bn2(self.thm_conv2(x_thm)))\n",
        "        x_thm = self.thm_pool(x_thm)\n",
        "        x_thm = self.thm_dropout2(x_thm)\n",
        "\n",
        "        x_thm = self.thm_global_pool(x_thm)\n",
        "        x_thm = self.thm_dropout3(x_thm)\n",
        "        x_thm = x_thm.view(batch_size, -1)\n",
        "\n",
        "        # ToF branch - reshape and process\n",
        "        x_tof = tof_input.view(batch_size, self.sequence_length, self.num_tof_sensors, 8, 8)\n",
        "        mask_reshaped = tof_mask.view(batch_size, self.sequence_length, self.num_tof_sensors, 8, 8)\n",
        "\n",
        "        # Apply mask and average over sequence length\n",
        "        x_tof = x_tof * mask_reshaped\n",
        "        x_tof = torch.mean(x_tof, dim=1)  # Average over sequence length\n",
        "\n",
        "        x_tof = F.relu(self.tof_bn1(self.tof_conv1(x_tof)))\n",
        "        x_tof = self.tof_pool1(x_tof)\n",
        "\n",
        "        x_tof = F.relu(self.tof_bn2(self.tof_conv2(x_tof)))\n",
        "        x_tof = self.tof_global_pool(x_tof)\n",
        "        x_tof = self.tof_dropout(x_tof)\n",
        "        x_tof = x_tof.view(batch_size, -1)\n",
        "\n",
        "        # ToF attention mechanism\n",
        "        tof_availability = torch.mean(tof_mask, dim=[1, 2], keepdim=True)\n",
        "        tof_attention = torch.tanh(self.tof_attention_fc1(tof_availability))\n",
        "        tof_attention = torch.sigmoid(self.tof_attention_fc2(tof_attention))\n",
        "        x_tof = x_tof * tof_attention.squeeze(1)\n",
        "\n",
        "        # Demographic branch\n",
        "        x_demo = F.relu(self.demo_bn1(self.demo_fc1(demo_input)))\n",
        "        x_demo = self.demo_dropout1(x_demo)\n",
        "        x_demo = F.relu(self.demo_bn2(self.demo_fc2(x_demo)))\n",
        "        x_demo = self.demo_dropout2(x_demo)\n",
        "\n",
        "        # Feature fusion with attention\n",
        "        concat_features = torch.cat([x_imu, x_thm, x_tof, x_demo], dim=1)\n",
        "\n",
        "        # Self-attention\n",
        "        attention_weights = F.softmax(self.feature_attention(concat_features), dim=1)\n",
        "        attended_features = concat_features * attention_weights\n",
        "\n",
        "        # Shared layers\n",
        "        shared = F.relu(self.shared_bn1(self.shared_fc1(attended_features)))\n",
        "        shared = self.shared_dropout1(shared)\n",
        "\n",
        "        shared = F.relu(self.shared_bn2(self.shared_fc2(shared)))\n",
        "        shared = self.shared_dropout2(shared)\n",
        "\n",
        "        shared = F.relu(self.shared_bn3(self.shared_fc3(shared)))\n",
        "        shared = self.shared_dropout3(shared)\n",
        "\n",
        "        # Output layers\n",
        "        binary_out = torch.sigmoid(self.binary_output(shared))\n",
        "        multiclass_out = F.softmax(self.multiclass_output(shared), dim=1)\n",
        "\n",
        "        return binary_out, multiclass_out\n",
        "\n",
        "class BFRBDataset(Dataset):\n",
        "    def __init__(self, X_dict, y_dict=None, sample_weights=None):\n",
        "        self.X_dict = X_dict\n",
        "        self.y_dict = y_dict\n",
        "        self.sample_weights = sample_weights\n",
        "        self.length = len(X_dict['imu_input'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {\n",
        "            'imu_input': torch.FloatTensor(self.X_dict['imu_input'][idx]),\n",
        "            'thm_input': torch.FloatTensor(self.X_dict['thm_input'][idx]),\n",
        "            'tof_input': torch.FloatTensor(self.X_dict['tof_input'][idx]),\n",
        "            'tof_mask': torch.FloatTensor(self.X_dict['tof_mask'][idx]),\n",
        "            'demo_input': torch.FloatTensor(self.X_dict['demo_input'][idx])\n",
        "        }\n",
        "\n",
        "        if self.y_dict is not None:\n",
        "            sample['binary_target'] = torch.FloatTensor([self.y_dict['binary_output'][idx]])\n",
        "            sample['multiclass_target'] = torch.LongTensor([self.y_dict['multiclass_output'][idx]])\n",
        "\n",
        "        if self.sample_weights is not None:\n",
        "            sample['binary_weight'] = torch.FloatTensor([self.sample_weights[0][idx]])\n",
        "            sample['multiclass_weight'] = torch.FloatTensor([self.sample_weights[1][idx]])\n",
        "\n",
        "        return sample\n",
        "\n",
        "def create_data_loaders(data, batch_size=16):\n",
        "    \"\"\"Create PyTorch DataLoaders from processed data\"\"\"\n",
        "\n",
        "    # Compute sample weights\n",
        "    binary_class_weight_dict = {0: BINARY_CLASS_WEIGHTS[0] * 1.5, 1: BINARY_CLASS_WEIGHTS[1]}\n",
        "    binary_sample_weights = compute_sample_weight(\n",
        "        binary_class_weight_dict,\n",
        "        data['y_train']['binary_output']\n",
        "    )\n",
        "\n",
        "    multiclass_class_weight_dict = {i: MULTICLASS_CLASS_WEIGHTS[i] for i in range(len(MULTICLASS_CLASS_WEIGHTS))}\n",
        "    multiclass_sample_weights = compute_sample_weight(\n",
        "        multiclass_class_weight_dict,\n",
        "        data['y_train']['multiclass_output']\n",
        "    )\n",
        "\n",
        "    sample_weights = [binary_sample_weights, multiclass_sample_weights]\n",
        "\n",
        "    train_dataset = BFRBDataset(data['X_train'], data['y_train'], sample_weights)\n",
        "    val_dataset = BFRBDataset(data['X_val'], data['y_val'])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4 if torch.cuda.is_available() else 0\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4 if torch.cuda.is_available() else 0\n",
        "    )\n",
        "\n",
        "    data_loaders = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "    if 'X_test' in data:\n",
        "        test_dataset = BFRBDataset(data['X_test'])\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=4 if torch.cuda.is_available() else 0\n",
        "        )\n",
        "        data_loaders['test'] = test_loader\n",
        "\n",
        "    return data_loaders\n",
        "\n",
        "class OptimizedThreshold:\n",
        "    def __init__(self, target_precision=0.75, patience=5, verbose=1):\n",
        "        self.target_precision = target_precision\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.best_threshold = 0.5\n",
        "        self.best_f1 = 0.0\n",
        "\n",
        "    def update(self, model, val_loader, epoch):\n",
        "        if epoch % self.patience == 0:\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            targets = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    imu_input = batch['imu_input'].to(device)\n",
        "                    thm_input = batch['thm_input'].to(device)\n",
        "                    tof_input = batch['tof_input'].to(device)\n",
        "                    tof_mask = batch['tof_mask'].to(device)\n",
        "                    demo_input = batch['demo_input'].to(device)\n",
        "                    binary_target = batch['binary_target'].to(device)\n",
        "\n",
        "                    binary_out, _ = model(imu_input, thm_input, tof_input, tof_mask, demo_input)\n",
        "\n",
        "                    predictions.extend(binary_out.squeeze().cpu().detach().numpy())\n",
        "                    targets.extend(binary_target.squeeze().cpu().detach().numpy())\n",
        "\n",
        "            # Find optimal threshold using precision-recall curve\n",
        "            precision, recall, thresholds = precision_recall_curve(targets, predictions)\n",
        "\n",
        "            # Calculate F1 scores for each threshold\n",
        "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "            # Find thresholds that meet minimum precision requirement\n",
        "            valid_indices = precision >= self.target_precision\n",
        "            if np.any(valid_indices):\n",
        "                valid_f1 = f1_scores[valid_indices]\n",
        "                valid_thresholds = thresholds[valid_indices[:-1]]  # thresholds array is 1 shorter\n",
        "\n",
        "                if len(valid_thresholds) > 0:\n",
        "                    best_idx = np.argmax(valid_f1)\n",
        "                    optimal_threshold = valid_thresholds[best_idx]\n",
        "                    optimal_f1 = valid_f1[best_idx]\n",
        "\n",
        "                    if optimal_f1 > self.best_f1:\n",
        "                        self.best_f1 = optimal_f1\n",
        "                        self.best_threshold = optimal_threshold\n",
        "\n",
        "                        if self.verbose:\n",
        "                            print(f\"\\nEpoch {epoch}: Updated optimal threshold to {optimal_threshold:.4f} \"\n",
        "                                  f\"(F1: {optimal_f1:.4f}, Precision: {precision[valid_indices][best_idx]:.4f})\")\n",
        "\n",
        "    def get_optimal_threshold(self):\n",
        "        return self.best_threshold\n",
        "\n",
        "def train_model_pytorch(model, data_loaders, epochs=80, learning_rate=2e-4, output_dir='pytorch_models'):\n",
        "    \"\"\"PyTorch training with precision optimization\"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Loss functions\n",
        "    binary_criterion = AsymmetricFocalLoss(alpha_pos=0.25, alpha_neg=0.75, gamma_pos=1.0, gamma_neg=4.0)\n",
        "    multiclass_criterion = SparseCategoricalFocalLoss(alpha=2.0, gamma=2.5, num_classes=18)\n",
        "\n",
        "    # Optimizer with weight decay\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.3, patience=6, min_lr=1e-8, verbose=True\n",
        "    )\n",
        "\n",
        "    # Threshold optimizer\n",
        "    threshold_optimizer = OptimizedThreshold(target_precision=0.75, patience=3, verbose=1)\n",
        "\n",
        "    # Metrics\n",
        "    train_metrics = {\n",
        "        'binary_f1': WeightedF1Score(),\n",
        "        'multiclass_f1': MacroF1Score(18),\n",
        "        'precision_at_recall': PrecisionAtRecall(0.95)\n",
        "    }\n",
        "\n",
        "    val_metrics = {\n",
        "        'binary_f1': WeightedF1Score(),\n",
        "        'multiclass_f1': MacroF1Score(18),\n",
        "        'precision_at_recall': PrecisionAtRecall(0.95)\n",
        "    }\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_binary_acc': [], 'val_binary_acc': [],\n",
        "        'train_multiclass_acc': [], 'val_multiclass_acc': [],\n",
        "        'train_binary_f1': [], 'val_binary_f1': [],\n",
        "        'train_multiclass_f1': [], 'val_multiclass_f1': [],\n",
        "        'train_precision_at_recall': [], 'val_precision_at_recall': []\n",
        "    }\n",
        "\n",
        "    best_val_precision = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 15\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(\"Starting precision-optimized model training...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{epochs}')\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        binary_correct = 0\n",
        "        multiclass_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Reset metrics\n",
        "        for metric in train_metrics.values():\n",
        "            metric.reset()\n",
        "\n",
        "        for batch_idx, batch in enumerate(data_loaders['train']):\n",
        "            # Move data to device\n",
        "            imu_input = batch['imu_input'].to(device)\n",
        "            thm_input = batch['thm_input'].to(device)\n",
        "            tof_input = batch['tof_input'].to(device)\n",
        "            tof_mask = batch['tof_mask'].to(device)\n",
        "            demo_input = batch['demo_input'].to(device)\n",
        "            binary_target = batch['binary_target'].to(device)\n",
        "            multiclass_target = batch['multiclass_target'].squeeze().to(device)\n",
        "\n",
        "            # Get sample weights if available\n",
        "            binary_weight = batch.get('binary_weight', torch.ones_like(binary_target)).to(device)\n",
        "            multiclass_weight = batch.get('multiclass_weight', torch.ones_like(multiclass_target.float())).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            binary_out, multiclass_out = model(imu_input, thm_input, tof_input, tof_mask, demo_input)\n",
        "\n",
        "            # Calculate losses with sample weights\n",
        "            binary_loss = binary_criterion(binary_out.squeeze(), binary_target.squeeze())\n",
        "            multiclass_loss = multiclass_criterion(multiclass_out, multiclass_target)\n",
        "\n",
        "            # Apply sample weights\n",
        "            binary_loss = binary_loss * binary_weight.squeeze().mean()\n",
        "            multiclass_loss = multiclass_loss * multiclass_weight.squeeze().mean()\n",
        "\n",
        "            # Combined loss with weights\n",
        "            total_loss = 0.6 * binary_loss + 0.4 * multiclass_loss\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.8)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "            # Binary accuracy\n",
        "            binary_pred = (binary_out.squeeze() > 0.5).float()\n",
        "            binary_correct += (binary_pred == binary_target.squeeze()).sum().item()\n",
        "\n",
        "            # Multiclass accuracy\n",
        "            multiclass_pred = torch.argmax(multiclass_out, dim=1)\n",
        "            multiclass_correct += (multiclass_pred == multiclass_target).sum().item()\n",
        "\n",
        "            total_samples += binary_target.size(0)\n",
        "\n",
        "            # Update metrics\n",
        "            train_metrics['binary_f1'].update(binary_out.squeeze(), binary_target.squeeze())\n",
        "            train_metrics['multiclass_f1'].update(multiclass_out, multiclass_target)\n",
        "            train_metrics['precision_at_recall'].update(binary_out.squeeze(), binary_target.squeeze())\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_loss = running_loss / len(data_loaders['train'])\n",
        "        binary_acc = binary_correct / total_samples\n",
        "        multiclass_acc = multiclass_correct / total_samples\n",
        "\n",
        "        train_binary_f1 = train_metrics['binary_f1'].compute()\n",
        "        train_multiclass_f1 = train_metrics['multiclass_f1'].compute()\n",
        "        train_precision_at_recall = train_metrics['precision_at_recall'].compute()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_binary_correct = 0\n",
        "        val_multiclass_correct = 0\n",
        "        val_total_samples = 0\n",
        "\n",
        "        # Reset validation metrics\n",
        "        for metric in val_metrics.values():\n",
        "            metric.reset()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loaders['val']:\n",
        "                # Move data to device\n",
        "                imu_input = batch['imu_input'].to(device)\n",
        "                thm_input = batch['thm_input'].to(device)\n",
        "                tof_input = batch['tof_input'].to(device)\n",
        "                tof_mask = batch['tof_mask'].to(device)\n",
        "                demo_input = batch['demo_input'].to(device)\n",
        "                binary_target = batch['binary_target'].to(device)\n",
        "                multiclass_target = batch['multiclass_target'].squeeze().to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                binary_out, multiclass_out = model(imu_input, thm_input, tof_input, tof_mask, demo_input)\n",
        "\n",
        "                # Calculate losses\n",
        "                binary_loss = binary_criterion(binary_out.squeeze(), binary_target.squeeze())\n",
        "                multiclass_loss = multiclass_criterion(multiclass_out, multiclass_target)\n",
        "                total_loss = 0.6 * binary_loss + 0.4 * multiclass_loss\n",
        "\n",
        "                val_loss += total_loss.item()\n",
        "\n",
        "                # Binary accuracy\n",
        "                binary_pred = (binary_out.squeeze() > 0.5).float()\n",
        "                val_binary_correct += (binary_pred == binary_target.squeeze()).sum().item()\n",
        "\n",
        "                # Multiclass accuracy\n",
        "                multiclass_pred = torch.argmax(multiclass_out, dim=1)\n",
        "                val_multiclass_correct += (multiclass_pred == multiclass_target).sum().item()\n",
        "\n",
        "                val_total_samples += binary_target.size(0)\n",
        "\n",
        "                # Update metrics\n",
        "                val_metrics['binary_f1'].update(binary_out.squeeze(), binary_target.squeeze())\n",
        "                val_metrics['multiclass_f1'].update(multiclass_out, multiclass_target)\n",
        "                val_metrics['precision_at_recall'].update(binary_out.squeeze(), binary_target.squeeze())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(data_loaders['val'])\n",
        "        val_binary_acc = val_binary_correct / val_total_samples\n",
        "        val_multiclass_acc = val_multiclass_correct / val_total_samples\n",
        "\n",
        "        val_binary_f1 = val_metrics['binary_f1'].compute()\n",
        "        val_multiclass_f1 = val_metrics['multiclass_f1'].compute()\n",
        "        val_precision_at_recall = val_metrics['precision_at_recall'].compute()\n",
        "\n",
        "        # Update threshold optimizer\n",
        "        threshold_optimizer.update(model, data_loaders['val'], epoch)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_precision_at_recall)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_binary_acc'].append(binary_acc)\n",
        "        history['val_binary_acc'].append(val_binary_acc)\n",
        "        history['train_multiclass_acc'].append(multiclass_acc)\n",
        "        history['val_multiclass_acc'].append(val_multiclass_acc)\n",
        "        history['train_binary_f1'].append(train_binary_f1)\n",
        "        history['val_binary_f1'].append(val_binary_f1)\n",
        "        history['train_multiclass_f1'].append(train_multiclass_f1)\n",
        "        history['val_multiclass_f1'].append(val_multiclass_f1)\n",
        "        history['train_precision_at_recall'].append(train_precision_at_recall)\n",
        "        history['val_precision_at_recall'].append(val_precision_at_recall)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "        print(f'Train Binary Acc: {binary_acc:.4f}, Val Binary Acc: {val_binary_acc:.4f}')\n",
        "        print(f'Train Multiclass Acc: {multiclass_acc:.4f}, Val Multiclass Acc: {val_multiclass_acc:.4f}')\n",
        "        print(f'Val Precision@Recall: {val_precision_at_recall:.4f}')\n",
        "\n",
        "        # Early stopping and model saving\n",
        "        if val_precision_at_recall > best_val_precision:\n",
        "            best_val_precision = val_precision_at_recall\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_precision': best_val_precision,\n",
        "                'threshold': threshold_optimizer.get_optimal_threshold()\n",
        "            }, os.path.join(output_dir, 'best_model.pth'))\n",
        "\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping triggered after {patience} epochs without improvement')\n",
        "            break\n",
        "\n",
        "    # Save optimal threshold for inference\n",
        "    optimal_threshold = threshold_optimizer.get_optimal_threshold()\n",
        "    threshold_info = {\n",
        "        'optimal_threshold': float(optimal_threshold),\n",
        "        'default_threshold': 0.5,\n",
        "        'precision_target': 0.75\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(os.path.join(output_dir, 'optimal_threshold.json'), 'w') as f:\n",
        "        json.dump(threshold_info, f, indent=2)\n",
        "\n",
        "    # Save final model and history\n",
        "    torch.save(model.state_dict(), os.path.join(output_dir, 'final_model.pth'))\n",
        "    pd.DataFrame(history).to_csv(os.path.join(output_dir, 'training_history.csv'), index=False)\n",
        "\n",
        "    print(f\"Training complete. Best validation precision@recall: {best_val_precision:.4f}\")\n",
        "    print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model_pytorch(model, val_loader):\n",
        "    \"\"\"PyTorch model evaluation with detailed metrics\"\"\"\n",
        "    print(\"Evaluating model...\")\n",
        "\n",
        "    model.eval()\n",
        "    binary_predictions = []\n",
        "    binary_targets = []\n",
        "    multiclass_predictions = []\n",
        "    multiclass_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            imu_input = batch['imu_input'].to(device)\n",
        "            thm_input = batch['thm_input'].to(device)\n",
        "            tof_input = batch['tof_input'].to(device)\n",
        "            tof_mask = batch['tof_mask'].to(device)\n",
        "            demo_input = batch['demo_input'].to(device)\n",
        "            binary_target = batch['binary_target'].to(device)\n",
        "            multiclass_target = batch['multiclass_target'].squeeze().to(device)\n",
        "\n",
        "            binary_out, multiclass_out = model(imu_input, thm_input, tof_input, tof_mask, demo_input)\n",
        "\n",
        "            binary_predictions.extend(binary_out.squeeze().cpu().detach().numpy())\n",
        "            binary_targets.extend(binary_target.squeeze().cpu().detach().numpy())\n",
        "            multiclass_predictions.extend(torch.argmax(multiclass_out, dim=1).cpu().detach().numpy())\n",
        "            multiclass_targets.extend(multiclass_target.cpu().detach().numpy())\n",
        "\n",
        "    # Binary classification analysis\n",
        "    binary_pred_classes = (np.array(binary_predictions) > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nBinary Classification Report:\")\n",
        "    print(classification_report(binary_targets, binary_pred_classes))\n",
        "    print(\"\\nBinary Confusion Matrix:\")\n",
        "    print(confusion_matrix(binary_targets, binary_pred_classes))\n",
        "\n",
        "    # Multiclass analysis\n",
        "    print(\"\\nMulticlass Classification Report:\")\n",
        "    print(classification_report(multiclass_targets, multiclass_predictions))\n",
        "\n",
        "    return {\n",
        "        'binary_predictions': binary_predictions,\n",
        "        'binary_targets': binary_targets,\n",
        "        'multiclass_predictions': multiclass_predictions,\n",
        "        'multiclass_targets': multiclass_targets\n",
        "    }\n",
        "\n",
        "def predict_and_save_pytorch(model, test_loader, sequence_ids, output_file):\n",
        "    \"\"\"PyTorch prediction with confidence scores\"\"\"\n",
        "    print(\"Generating predictions...\")\n",
        "\n",
        "    model.eval()\n",
        "    binary_predictions = []\n",
        "    multiclass_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            imu_input = batch['imu_input'].to(device)\n",
        "            thm_input = batch['thm_input'].to(device)\n",
        "            tof_input = batch['tof_input'].to(device)\n",
        "            tof_mask = batch['tof_mask'].to(device)\n",
        "            demo_input = batch['demo_input'].to(device)\n",
        "\n",
        "            binary_out, multiclass_out = model(imu_input, thm_input, tof_input, tof_mask, demo_input)\n",
        "\n",
        "            binary_predictions.extend(binary_out.squeeze().cpu().numpy())\n",
        "            multiclass_predictions.extend(multiclass_out.cpu().numpy())\n",
        "\n",
        "    binary_predictions = np.array(binary_predictions)\n",
        "    multiclass_predictions = np.array(multiclass_predictions)\n",
        "\n",
        "    # Process predictions\n",
        "    binary_classes = (binary_predictions > 0.5).astype(int)\n",
        "    binary_confidence = np.maximum(binary_predictions, 1-binary_predictions)\n",
        "\n",
        "    multiclass_classes = np.argmax(multiclass_predictions, axis=1)\n",
        "    multiclass_confidence = np.max(multiclass_predictions, axis=1)\n",
        "\n",
        "    # Gesture mapping\n",
        "    gesture_map = {\n",
        "        0: \"non_target\",\n",
        "        1: \"above_ear_pull_hair\",\n",
        "        2: \"forehead_pull_hairline\",\n",
        "        3: \"forehead_scratch\",\n",
        "        4: \"eyebrow_pull_hair\",\n",
        "        5: \"eyelash_pull_hair\",\n",
        "        6: \"neck_pinch_skin\",\n",
        "        7: \"neck_scratch\",\n",
        "        8: \"cheek_pinch_skin\",\n",
        "        9: \"drink_from_bottle\",\n",
        "        10: \"glasses_on_off\",\n",
        "        11: \"pull_air_toward_face\",\n",
        "        12: \"pinch_knee_leg_skin\",\n",
        "        13: \"scratch_knee_leg_skin\",\n",
        "        14: \"write_name_on_leg\",\n",
        "        15: \"text_on_phone\",\n",
        "        16: \"feel_around_tray\",\n",
        "        17: \"write_name_in_air\"\n",
        "    }\n",
        "\n",
        "    output_df = pd.DataFrame({\n",
        "        'sequence_id': sequence_ids,\n",
        "        'is_target': binary_classes,\n",
        "        'binary_confidence': binary_confidence,\n",
        "        'gesture_class': multiclass_classes,\n",
        "        'multiclass_confidence': multiclass_confidence,\n",
        "        'gesture': [gesture_map.get(cls, f\"gesture_{cls}\") for cls in multiclass_classes]\n",
        "    })\n",
        "\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "def save_model_pytorch(model, model_dir):\n",
        "    \"\"\"PyTorch model saving with metadata\"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save model state dict\n",
        "    torch.save(model.state_dict(), os.path.join(model_dir, 'model_state_dict.pth'))\n",
        "\n",
        "    # Save entire model\n",
        "    torch.save(model, os.path.join(model_dir, 'complete_model.pth'))\n",
        "\n",
        "    # Save model metadata\n",
        "    metadata = {\n",
        "        'model_type': 'PyTorch_Multimodal_BFRB_CNN',\n",
        "        'input_shape': {\n",
        "            'imu': (MAX_SEQUENCE_LENGTH, NUM_IMU_FEATURES),\n",
        "            'thermopile': (MAX_SEQUENCE_LENGTH, NUM_THERMOPILE),\n",
        "            'tof': (MAX_SEQUENCE_LENGTH, NUM_TOF_SENSORS * TOF_PIXELS_PER_SENSOR),\n",
        "            'demographic': (NUM_DEMOGRAPHIC_FEATURES,)\n",
        "        },\n",
        "        'num_classes': 18,\n",
        "        'improvements': [\n",
        "            'asymmetric_focal_loss', 'data_augmentation', 'sample_weights',\n",
        "            'precision_optimization', 'attention_mechanism'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    import json\n",
        "    with open(os.path.join(model_dir, 'model_metadata.json'), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"PyTorch model saved to {model_dir} with metadata\")\n",
        "\n",
        "def load_model_pytorch(model_dir):\n",
        "    \"\"\"PyTorch model loading\"\"\"\n",
        "    try:\n",
        "        # Try loading complete model first\n",
        "        model = torch.load(os.path.join(model_dir, 'complete_model.pth'), map_location=device)\n",
        "        print(f\"Loaded complete model from {model_dir}\")\n",
        "    except:\n",
        "        # Load from state dict\n",
        "        model = MultimodalBFRBCNN(num_gestures=18)\n",
        "        model.load_state_dict(torch.load(os.path.join(model_dir, 'model_state_dict.pth'), map_location=device))\n",
        "        print(f\"Loaded model from state dict: {model_dir}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution with PyTorch precision optimization\"\"\"\n",
        "\n",
        "    # File paths\n",
        "    train_file = 'train.csv'\n",
        "    train_demo_file = 'train_demographics.csv'\n",
        "    test_file = 'test.csv'\n",
        "    test_demo_file = 'test_demographics.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    data = load_and_preprocess_data(train_file, train_demo_file, test_file, test_demo_file)\n",
        "\n",
        "    # Create PyTorch data loaders\n",
        "    data_loaders = create_data_loaders(data, batch_size=16)\n",
        "\n",
        "    # Build PyTorch model\n",
        "    model = MultimodalBFRBCNN(num_gestures=18)\n",
        "\n",
        "    print(f\"\\nPyTorch Model Summary:\")\n",
        "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "    # Train with PyTorch\n",
        "    if 'train' in data_loaders and 'val' in data_loaders:\n",
        "        model, history = train_model_pytorch(\n",
        "            model, data_loaders, epochs=80, learning_rate=2e-4,\n",
        "            output_dir='pytorch_precision_optimized_bfrb_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        if 'val' in data_loaders:\n",
        "            evaluate_model_pytorch(model, data_loaders['val'])\n",
        "\n",
        "    # Generate predictions\n",
        "    if 'test' in data_loaders:\n",
        "        predict_and_save_pytorch(model, data_loaders['test'], data.get('sequence_ids', []),\n",
        "                                'pytorch_precision_optimized_predictions.csv')\n",
        "\n",
        "    # Save model\n",
        "    save_model_pytorch(model, 'pytorch_precision_optimized_bfrb_model')\n",
        "\n",
        "    print(\"PyTorch BFRB detection pipeline complete with precision optimization\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c cmi-detect-behavior-with-sensor-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDOdPDlzu-RM",
        "outputId": "60e9b841-d36b-47bc-f3ef-9e08fa64d9a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading cmi-detect-behavior-with-sensor-data.zip to /content\n",
            " 87% 155M/178M [00:00<00:00, 192MB/s]\n",
            "100% 178M/178M [00:00<00:00, 213MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"cmi-detect-behavior-with-sensor-data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ],
      "metadata": {
        "id": "TftqRzIgu_-M"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}
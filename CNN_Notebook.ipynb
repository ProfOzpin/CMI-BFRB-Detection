{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMf60jsXxhdSdYxas9J/GUo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "l-dlnkMZ_kwJ",
        "outputId": "290826b8-a0ff-4370-db76-ac3f13600cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuML available - GPU acceleration enabled\n",
            "Loading data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "['train.csv'] could not be resolved to any files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-833629035>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-833629035>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m     \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m     data = load_and_preprocess_data(\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mtrain_demo_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-833629035>\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(train_file, train_demographics_file, test_file, test_demographics_file)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Load training data with a limit of 1000 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCUML_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mtrain_demo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_demographics_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/io/csv.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, prefix, mangle_dupe_cols, dtype, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, dayfirst, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, comment, delim_whitespace, byte_range, storage_options, bytes_per_thread)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mbytes_per_thread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mioutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_BYTES_PER_THREAD_DEFAULT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     filepath_or_buffer = ioutils.get_reader_filepath_or_buffer(\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mpath_or_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0miotypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/ioutils.py\u001b[0m in \u001b[0;36mget_reader_filepath_or_buffer\u001b[0;34m(path_or_data, mode, fs, iotypes, allow_raw_text_input, storage_options, bytes_per_thread, warn_on_raw_text_input, warn_meta, expand_dir_pattern, prefetch_options)\u001b[0m\n\u001b[1;32m   1882\u001b[0m                     \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".json{c}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompression_extensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 ):\n\u001b[0;32m-> 1884\u001b[0;31m                     raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1885\u001b[0m                         \u001b[0;34mf\"{input_sources} could not be resolved to any files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m                     )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: ['train.csv'] could not be resolved to any files"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import cuML for GPU acceleration\n",
        "try:\n",
        "    import cudf\n",
        "    import cuml\n",
        "    from cuml.preprocessing import StandardScaler as cuMLStandardScaler\n",
        "    from cuml.model_selection import train_test_split as cuml_train_test_split\n",
        "    CUML_AVAILABLE = True\n",
        "    print(\"cuML available - GPU acceleration enabled\")\n",
        "except ImportError:\n",
        "    CUML_AVAILABLE = False\n",
        "    print(\"cuML not available - using CPU preprocessing\")\n",
        "\n",
        "# Set mixed precision for GPU optimization\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Configure GPU memory growth\n",
        "# Removed GPU memory growth configuration as it conflicts with cuML\n",
        "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "# if len(physical_devices) > 0:\n",
        "#     for gpu in physical_devices:\n",
        "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
        "#     print(f\"GPU configuration complete: {len(physical_devices)} GPU(s) found\")\n",
        "# else:\n",
        "#     print(\"No GPU found - using CPU\")\n",
        "\n",
        "\n",
        "# Constants for the model\n",
        "MAX_SEQUENCE_LENGTH = 50  # Maximum sequence length\n",
        "NUM_IMU_FEATURES = 7      # acc_x, acc_y, acc_z, rot_w, rot_x, rot_y, rot_z\n",
        "NUM_THERMOPILE = 5        # thm_1 through thm_5\n",
        "NUM_TOF_SENSORS = 5       # Time of Flight sensors\n",
        "TOF_PIXELS_PER_SENSOR = 64  # 8x8 grid per ToF sensor\n",
        "NUM_DEMOGRAPHIC_FEATURES = 7  # adult_child, age, sex, handedness, height, shoulder_to_wrist, elbow_to_wrist\n",
        "\n",
        "def load_and_preprocess_data(train_file, train_demographics_file, test_file=None, test_demographics_file=None):\n",
        "    \"\"\"\n",
        "    Load and preprocess the BFRB dataset with sensor and demographic data\n",
        "\n",
        "    Args:\n",
        "        train_file: Path to training CSV file\n",
        "        train_demographics_file: Path to training demographics CSV file\n",
        "        test_file: Optional path to test CSV file\n",
        "        test_demographics_file: Optional path to test demographics CSV file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing processed data for model training and evaluation\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    # Load training data with a limit of 1000 rows\n",
        "    if CUML_AVAILABLE:\n",
        "        train_df = cudf.read_csv(train_file)\n",
        "        train_demo_df = cudf.read_csv(train_demographics_file)\n",
        "    else:\n",
        "        train_df = pd.read_csv(train_file)\n",
        "        train_demo_df = pd.read_csv(train_demographics_file)\n",
        "\n",
        "    # Process training sequences\n",
        "    print(\"Processing training sequences...\")\n",
        "    train_sequences = process_sequences(train_df, train_demo_df)\n",
        "\n",
        "    # Prepare training data for model\n",
        "    print(\"Preparing training data for model...\")\n",
        "    X_train, y_train = prepare_model_data(train_sequences)\n",
        "\n",
        "    # Always create train/validation split for training\n",
        "    if CUML_AVAILABLE:\n",
        "        # Convert dictionaries to cudf DataFrames or concatenate to single arrays for cuML split\n",
        "        # For simplicity and compatibility with the model's dictionary input, we'll use pandas split\n",
        "        # or manually split the indices and then subset the dictionaries.\n",
        "        # Let's stick to the manual index splitting approach which works for both pandas and cuML arrays if converted.\n",
        "\n",
        "        # Convert CuPy arrays in X_train and y_train to NumPy for sklearn split\n",
        "        X_train_np = {key: value.get() if hasattr(value, 'get') else value for key, value in X_train.items()}\n",
        "        y_train_np = {key: value.get() if hasattr(value, 'get') else value for key, value in y_train.items()}\n",
        "\n",
        "\n",
        "        train_indices, val_indices = train_test_split(\n",
        "            range(len(X_train_np['imu_input'])), test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train_split = {key: value[train_indices] for key, value in X_train.items()}\n",
        "        X_val = {key: value[val_indices] for key, value in X_train.items()}\n",
        "        y_train_split = {key: value[train_indices] for key, value in y_train.items()}\n",
        "        y_val = {key: value[val_indices] for key, value in y_train.items()}\n",
        "\n",
        "\n",
        "    else:\n",
        "        # For dictionary inputs, we need to split each component\n",
        "        train_indices, val_indices = train_test_split(\n",
        "            range(len(X_train['imu_input'])), test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        X_train_split = {key: value[train_indices] for key, value in X_train.items()}\n",
        "        X_val = {key: value[val_indices] for key, value in X_train.items()}\n",
        "        y_train_split = {key: value[train_indices] for key, value in y_train.items()}\n",
        "        y_val = {key: value[val_indices] for key, value in y_train.items()}\n",
        "\n",
        "    # Prepare return dictionary with training and validation data\n",
        "    data_dict = {\n",
        "        'X_train': X_train_split,\n",
        "        'y_train': y_train_split,\n",
        "        'X_val': X_val,\n",
        "        'y_val': y_val\n",
        "    }\n",
        "\n",
        "    # Load and process test data if provided\n",
        "    if test_file and test_demographics_file:\n",
        "        print(\"Loading test data...\")\n",
        "        if CUML_AVAILABLE:\n",
        "            test_df = cudf.read_csv(test_file)\n",
        "            test_demo_df = cudf.read_csv(test_demographics_file)\n",
        "        else:\n",
        "            test_df = pd.read_csv(test_file)\n",
        "            test_demo_df = pd.read_csv(test_demographics_file)\n",
        "\n",
        "        print(\"Processing test sequences...\")\n",
        "        test_sequences = process_sequences(test_df, test_demo_df, is_train=False)\n",
        "\n",
        "        print(\"Preparing test data for model...\")\n",
        "        X_test, sequence_ids = prepare_model_data(test_sequences, is_train=False)\n",
        "\n",
        "        # Add test data to dictionary\n",
        "        data_dict['X_test'] = X_test\n",
        "        data_dict['sequence_ids'] = sequence_ids\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "def process_sequences(df, demo_df, is_train=True):\n",
        "    \"\"\"\n",
        "    Process raw dataframe into sequence data with demographic information\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "\n",
        "    # Get unique sequence IDs and convert to host array if using cuML\n",
        "    if CUML_AVAILABLE:\n",
        "        sequence_ids = df['sequence_id'].unique().values_host\n",
        "    else:\n",
        "        sequence_ids = df['sequence_id'].unique()\n",
        "\n",
        "    for seq_id in sequence_ids:\n",
        "        seq_data = df[df['sequence_id'] == seq_id]\n",
        "\n",
        "        # Skip sequence if empty\n",
        "        if len(seq_data) == 0:\n",
        "            print(f\"Warning: Empty sequence data for sequence ID {seq_id}\")\n",
        "            continue\n",
        "\n",
        "        # Convert to pandas if using cuML for easier processing\n",
        "        if CUML_AVAILABLE:\n",
        "            seq_data = seq_data.to_pandas()\n",
        "            demo_df_pandas = demo_df.to_pandas()\n",
        "        else:\n",
        "            demo_df_pandas = demo_df\n",
        "\n",
        "        # Get subject ID for this sequence\n",
        "        subject_id = seq_data['subject'].iloc[0]\n",
        "\n",
        "        # Get demographic data for this subject\n",
        "        subject_demo = demo_df_pandas[demo_df_pandas['subject'] == subject_id]\n",
        "\n",
        "        if len(subject_demo) == 0:\n",
        "            print(f\"Warning: No demographic data found for subject {subject_id}\")\n",
        "            continue\n",
        "\n",
        "        # Extract demographic features and handle potential nulls\n",
        "        demo_features = [\n",
        "            subject_demo['adult_child'].fillna(0).iloc[0],\n",
        "            subject_demo['age'].fillna(0).iloc[0],\n",
        "            subject_demo['sex'].fillna(0).iloc[0],\n",
        "            subject_demo['handedness'].fillna(0).iloc[0],\n",
        "            subject_demo['height_cm'].fillna(0).iloc[0],\n",
        "            subject_demo['shoulder_to_wrist_cm'].fillna(0).iloc[0],\n",
        "            subject_demo['elbow_to_wrist_cm'].fillna(0).iloc[0]\n",
        "        ]\n",
        "\n",
        "        # Extract sequence features\n",
        "        imu_cols = [col for col in seq_data.columns if col.startswith('acc_') or col.startswith('rot_')]\n",
        "        thm_cols = [col for col in seq_data.columns if col.startswith('thm_')]\n",
        "        tof_cols = [col for col in seq_data.columns if col.startswith('tof_')]\n",
        "\n",
        "        # Handle sequence length\n",
        "        if len(seq_data) < MAX_SEQUENCE_LENGTH:\n",
        "            padding_needed = MAX_SEQUENCE_LENGTH - len(seq_data)\n",
        "            last_row = seq_data.iloc[-1:].copy()\n",
        "            for _ in range(padding_needed):\n",
        "                seq_data = pd.concat([seq_data, last_row])\n",
        "        elif len(seq_data) > MAX_SEQUENCE_LENGTH:\n",
        "            seq_data = seq_data.iloc[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        # Extract data and convert to numpy arrays\n",
        "        imu_data = seq_data[imu_cols].fillna(0).values\n",
        "        thm_data = seq_data[thm_cols].fillna(0).values\n",
        "        tof_data = seq_data[tof_cols].fillna(0).values\n",
        "\n",
        "        # Create mask for missing ToF data\n",
        "        tof_mask = (tof_data != -1).astype(np.float32)\n",
        "        tof_data = np.where(tof_data == -1, 0, tof_data)\n",
        "\n",
        "        sequence = {\n",
        "            'sequence_id': seq_id,\n",
        "            'imu_data': imu_data,\n",
        "            'thm_data': thm_data,\n",
        "            'tof_data': tof_data,\n",
        "            'tof_mask': tof_mask,\n",
        "            'demographic': demo_features,\n",
        "        }\n",
        "\n",
        "        # Add target values for training data\n",
        "        if is_train:\n",
        "            sequence_type = seq_data['sequence_type'].iloc[0]\n",
        "            gesture = seq_data['gesture'].iloc[0]\n",
        "            binary_target = 1 if sequence_type == 'target' else 0\n",
        "\n",
        "            sequence['binary_target'] = binary_target\n",
        "            sequence['gesture'] = gesture\n",
        "\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    return sequences\n",
        "\n",
        "def prepare_model_data(sequences, is_train=True):\n",
        "    \"\"\"\n",
        "    Prepare sequences for model input\n",
        "\n",
        "    Args:\n",
        "        sequences: List of processed sequence dictionaries\n",
        "        is_train: Boolean indicating if this is training data\n",
        "\n",
        "    Returns:\n",
        "        Model inputs and targets (for training) or sequence IDs (for testing)\n",
        "    \"\"\"\n",
        "    # Initialize lists for inputs\n",
        "    imu_data = []\n",
        "    thm_data = []\n",
        "    tof_data = []\n",
        "    tof_mask = []\n",
        "    demo_data = []\n",
        "\n",
        "    # Targets for training data\n",
        "    binary_targets = []\n",
        "    gesture_targets = []\n",
        "    sequence_ids = []\n",
        "\n",
        "    # Process each sequence\n",
        "    for seq in sequences:\n",
        "        imu_data.append(seq['imu_data'])\n",
        "        thm_data.append(seq['thm_data'])\n",
        "        tof_data.append(seq['tof_data'])\n",
        "        tof_mask.append(seq['tof_mask'])\n",
        "        demo_data.append(seq['demographic'])\n",
        "        sequence_ids.append(seq['sequence_id'])\n",
        "\n",
        "        if is_train:\n",
        "            binary_targets.append(seq['binary_target'])\n",
        "            gesture_targets.append(seq['gesture'])\n",
        "\n",
        "    # Helper function to convert CuPy arrays to NumPy\n",
        "    def cupy_to_numpy(data):\n",
        "        \"\"\"Convert CuPy arrays to NumPy arrays if necessary\"\"\"\n",
        "        if hasattr(data, 'get'):  # CuPy array\n",
        "            return data.get()\n",
        "        elif isinstance(data, list):\n",
        "            return [cupy_to_numpy(item) for item in data]\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "    # Convert CuPy arrays to NumPy arrays if necessary\n",
        "    imu_data = [cupy_to_numpy(item) for item in imu_data]\n",
        "    thm_data = [cupy_to_numpy(item) for item in thm_data]\n",
        "    tof_data = [cupy_to_numpy(item) for item in tof_data]\n",
        "    tof_mask = [cupy_to_numpy(item) for item in tof_mask]\n",
        "    demo_data = [cupy_to_numpy(item) for item in demo_data]\n",
        "\n",
        "    # Now safely convert to numpy arrays\n",
        "    imu_data = np.array(imu_data)\n",
        "    thm_data = np.array(thm_data)\n",
        "    tof_data = np.array(tof_data)\n",
        "    tof_mask = np.array(tof_mask)\n",
        "    demo_data = np.array(demo_data)\n",
        "\n",
        "    # Use sklearn scalers for TensorFlow compatibility\n",
        "    from sklearn.preprocessing import StandardScaler as SKStandardScaler\n",
        "    imu_scaler = SKStandardScaler()\n",
        "    thm_scaler = SKStandardScaler()\n",
        "    demo_scaler = SKStandardScaler()\n",
        "\n",
        "    # Reshape for scaling\n",
        "    imu_shape = imu_data.shape\n",
        "    thm_shape = thm_data.shape\n",
        "\n",
        "    imu_flat = imu_data.reshape(-1, imu_shape[2])\n",
        "    thm_flat = thm_data.reshape(-1, thm_shape[2])\n",
        "\n",
        "    # Fit and transform\n",
        "    imu_flat = imu_scaler.fit_transform(imu_flat)\n",
        "    thm_flat = thm_scaler.fit_transform(thm_flat)\n",
        "    demo_data = demo_scaler.fit_transform(demo_data)\n",
        "\n",
        "    # Reshape back\n",
        "    imu_data = imu_flat.reshape(imu_shape)\n",
        "    thm_data = thm_flat.reshape(thm_shape)\n",
        "\n",
        "    # Normalize ToF data (valid values range from 0-254)\n",
        "    # Only normalize non-masked values\n",
        "    tof_data = np.where(tof_mask == 1, tof_data / 254.0, 0)\n",
        "\n",
        "    # Create model inputs dictionary\n",
        "    X = {\n",
        "        'imu_input': imu_data,\n",
        "        'thm_input': thm_data,\n",
        "        'tof_input': tof_data,\n",
        "        'tof_mask': tof_mask,\n",
        "        'demo_input': demo_data\n",
        "    }\n",
        "\n",
        "    if is_train:\n",
        "        # Convert targets to numpy arrays with CuPy handling\n",
        "        binary_targets = cupy_to_numpy(binary_targets)\n",
        "        gesture_targets = cupy_to_numpy(gesture_targets)\n",
        "\n",
        "        binary_targets = np.array(binary_targets)\n",
        "\n",
        "        # Handle gesture targets\n",
        "        label_encoder = LabelEncoder()\n",
        "        gesture_targets = label_encoder.fit_transform(gesture_targets)\n",
        "        gesture_targets = np.array(gesture_targets)\n",
        "\n",
        "        # Create targets dictionary\n",
        "        y = {\n",
        "            'binary_output': binary_targets,\n",
        "            'multiclass_output': gesture_targets\n",
        "        }\n",
        "\n",
        "        return X, y\n",
        "    else:\n",
        "        return X, sequence_ids\n",
        "\n",
        "def build_multimodal_cnn(\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    num_imu_features=NUM_IMU_FEATURES,\n",
        "    num_thermopile=NUM_THERMOPILE,\n",
        "    num_tof_sensors=NUM_TOF_SENSORS,\n",
        "    tof_pixels=TOF_PIXELS_PER_SENSOR,\n",
        "    num_demographic=NUM_DEMOGRAPHIC_FEATURES,\n",
        "    num_gestures=19  # 8 BFRB + 10 non-BFRB + 1 non-target\n",
        "):\n",
        "    \"\"\"\n",
        "    Build multimodal CNN for BFRB detection\n",
        "\n",
        "    Args:\n",
        "        sequence_length: Length of input sequences\n",
        "        num_imu_features: Number of IMU features\n",
        "        num_thermopile: Number of thermopile sensors\n",
        "        num_tof_sensors: Number of ToF sensors\n",
        "        tof_pixels: Pixels per ToF sensor\n",
        "        num_demographic: Number of demographic features\n",
        "        num_gestures: Number of gesture classes\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    # Input layers\n",
        "    imu_input = Input(shape=(sequence_length, num_imu_features), name='imu_input')\n",
        "    thm_input = Input(shape=(sequence_length, num_thermopile), name='thm_input')\n",
        "    tof_input = Input(shape=(sequence_length, num_tof_sensors * tof_pixels), name='tof_input')\n",
        "    tof_mask = Input(shape=(sequence_length, num_tof_sensors * tof_pixels), name='tof_mask')\n",
        "    demo_input = Input(shape=(num_demographic,), name='demo_input')\n",
        "\n",
        "    # IMU branch - 1D CNN\n",
        "    x_imu = layers.Conv1D(32, 3, activation='relu', padding='same')(imu_input)\n",
        "    x_imu = layers.BatchNormalization()(x_imu)\n",
        "    x_imu = layers.MaxPooling1D(2)(x_imu)\n",
        "\n",
        "    x_imu = layers.Conv1D(64, 3, activation='relu', padding='same')(x_imu)\n",
        "    x_imu = layers.BatchNormalization()(x_imu)\n",
        "    x_imu = layers.MaxPooling1D(2)(x_imu)\n",
        "\n",
        "    x_imu = layers.Conv1D(128, 3, activation='relu', padding='same')(x_imu)\n",
        "    x_imu = layers.BatchNormalization()(x_imu)\n",
        "    x_imu = layers.GlobalAveragePooling1D()(x_imu)\n",
        "    x_imu = layers.Dropout(0.3)(x_imu)\n",
        "\n",
        "    # Thermopile branch - 1D CNN\n",
        "    x_thm = layers.Conv1D(16, 3, activation='relu', padding='same')(thm_input)\n",
        "    x_thm = layers.BatchNormalization()(x_thm)\n",
        "    x_thm = layers.MaxPooling1D(2)(x_thm)\n",
        "\n",
        "    x_thm = layers.Conv1D(32, 3, activation='relu', padding='same')(x_thm)\n",
        "    x_thm = layers.BatchNormalization()(x_thm)\n",
        "    x_thm = layers.GlobalAveragePooling1D()(x_thm)\n",
        "    x_thm = layers.Dropout(0.3)(x_thm)\n",
        "\n",
        "    # ToF branch - Reshape and apply 3D CNN\n",
        "    # Reshape to (batch, time, sensors, 8, 8, 1) for 3D CNN\n",
        "    x_tof = layers.Reshape((sequence_length, num_tof_sensors, 8, 8))(tof_input)\n",
        "\n",
        "    # Apply mask by multiplying with reshaped mask\n",
        "    mask_reshaped = layers.Reshape((sequence_length, num_tof_sensors, 8, 8))(tof_mask)\n",
        "    x_tof = layers.Multiply()([x_tof, mask_reshaped])\n",
        "\n",
        "    # Expand dimensions for conv3d\n",
        "    x_tof = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(x_tof)\n",
        "\n",
        "    # Apply 3D CNN using TimeDistributed to process each time step\n",
        "    x_tof = layers.TimeDistributed(\n",
        "        layers.Conv3D(16, (1, 3, 3), activation='relu', padding='same')\n",
        "    )(x_tof)\n",
        "    x_tof = layers.TimeDistributed(layers.BatchNormalization())(x_tof)\n",
        "    x_tof = layers.TimeDistributed(layers.MaxPooling3D(pool_size=(1, 2, 2)))(x_tof)\n",
        "\n",
        "    x_tof = layers.TimeDistributed(\n",
        "        layers.Conv3D(32, (1, 3, 3), activation='relu', padding='same')\n",
        "    )(x_tof)\n",
        "    x_tof = layers.TimeDistributed(layers.BatchNormalization())(x_tof)\n",
        "\n",
        "    # Reshape and apply 1D convolution across time\n",
        "    x_tof = layers.TimeDistributed(\n",
        "        layers.Reshape((-1,))\n",
        "    )(x_tof)\n",
        "\n",
        "    x_tof = layers.Conv1D(64, 3, activation='relu', padding='same')(x_tof)\n",
        "    x_tof = layers.BatchNormalization()(x_tof)\n",
        "    x_tof = layers.GlobalAveragePooling1D()(x_tof)\n",
        "    x_tof = layers.Dropout(0.3)(x_tof)\n",
        "\n",
        "    # Demographic branch - Dense network\n",
        "    x_demo = layers.Dense(32, activation='relu')(demo_input)\n",
        "    x_demo = layers.BatchNormalization()(x_demo)\n",
        "    x_demo = layers.Dropout(0.3)(x_demo)\n",
        "\n",
        "    # Weighted fusion of branches based on ToF mask availability\n",
        "    # Calculate fraction of valid ToF data\n",
        "    tof_availability = layers.Lambda(\n",
        "        lambda x: tf.reduce_mean(x, axis=[1, 2])\n",
        "    )(tof_mask)\n",
        "\n",
        "    # Create fusion weights - if ToF data available, use it more\n",
        "    tof_weight = layers.Lambda(lambda x: x)(tof_availability)\n",
        "    imu_weight = layers.Lambda(lambda x: tf.ones_like(x))(tof_availability)\n",
        "    thm_weight = layers.Lambda(lambda x: tf.ones_like(x))(tof_availability)\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    total_weight = layers.Add()([imu_weight, thm_weight, tof_weight])\n",
        "    imu_weight = layers.Lambda(lambda x: x[0]/x[1])([imu_weight, total_weight])\n",
        "    thm_weight = layers.Lambda(lambda x: x[0]/x[1])([thm_weight, total_weight])\n",
        "    tof_weight = layers.Lambda(lambda x: x[0]/x[1])([tof_weight, total_weight])\n",
        "\n",
        "    # Reshape weights for multiplication\n",
        "    imu_weight = layers.Reshape((1,))(imu_weight)\n",
        "    thm_weight = layers.Reshape((1,))(thm_weight)\n",
        "    tof_weight = layers.Reshape((1,))(tof_weight)\n",
        "\n",
        "    # Apply weights by broadcasting\n",
        "    x_imu_weighted = layers.Multiply()([x_imu, imu_weight])\n",
        "    x_thm_weighted = layers.Multiply()([x_thm, thm_weight])\n",
        "    x_tof_weighted = layers.Multiply()([x_tof, tof_weight])\n",
        "\n",
        "    # Concatenate all features\n",
        "    merged = layers.Concatenate()([x_imu_weighted, x_thm_weighted, x_tof_weighted, x_demo])\n",
        "\n",
        "    # Common layers\n",
        "    shared = layers.Dense(128, activation='relu')(merged)\n",
        "    shared = layers.BatchNormalization()(shared)\n",
        "    shared = layers.Dropout(0.5)(shared)\n",
        "\n",
        "    # Output layers\n",
        "    binary_output = layers.Dense(1, activation='sigmoid', name='binary_output')(shared)\n",
        "    multiclass_output = layers.Dense(num_gestures, activation='softmax', name='multiclass_output')(shared)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(\n",
        "        inputs=[imu_input, thm_input, tof_input, tof_mask, demo_input],\n",
        "        outputs=[binary_output, multiclass_output]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Custom F1 Score implementation for TensorFlow\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = tf.keras.metrics.Precision() # Use fully qualified name\n",
        "        self.recall = tf.keras.metrics.Recall() # Use fully qualified name\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Threshold predictions to get binary values\n",
        "        y_pred_binary = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
        "\n",
        "        # Update precision and recall\n",
        "        self.precision.update_state(y_true, y_pred_binary, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred_binary, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        # Calculate F1 from precision and recall\n",
        "        p = self.precision.result()\n",
        "        r = self.recall.result()\n",
        "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "# Custom Macro F1 for multi-class classification\n",
        "class MacroF1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, num_classes, name='macro_f1_score', **kwargs):\n",
        "        super(MacroF1Score, self).__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Fix: Use explicit parameter names\n",
        "        self.true_positives = self.add_weight(\n",
        "            shape=(num_classes,),\n",
        "            name='true_positives',\n",
        "            initializer='zeros'\n",
        "        )\n",
        "        self.false_positives = self.add_weight(\n",
        "            shape=(num_classes,),\n",
        "            name='false_positives',\n",
        "            initializer='zeros'\n",
        "        )\n",
        "        self.false_negatives = self.add_weight(\n",
        "            shape=(num_classes,),\n",
        "            name='false_negatives',\n",
        "            initializer='zeros'\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Convert predictions to one-hot\n",
        "        y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "        y_pred_one_hot = tf.one_hot(y_pred_classes, self.num_classes)\n",
        "\n",
        "        # Convert sparse y_true to one-hot\n",
        "        if len(y_true.shape) == 1 or y_true.shape[1] == 1:\n",
        "            y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), self.num_classes)\n",
        "        else:\n",
        "            y_true_one_hot = y_true\n",
        "\n",
        "        # Calculate true positives, false positives, false negatives\n",
        "        tp = tf.reduce_sum(y_true_one_hot * y_pred_one_hot, axis=0)\n",
        "        fp = tf.reduce_sum((1 - y_true_one_hot) * y_pred_one_hot, axis=0)\n",
        "        fn = tf.reduce_sum(y_true_one_hot * (1 - y_pred_one_hot), axis=0)\n",
        "\n",
        "        # Update states\n",
        "        self.true_positives.assign_add(tp)\n",
        "        self.false_positives.assign_add(fp)\n",
        "        self.false_negatives.assign_add(fn)\n",
        "\n",
        "    def result(self):\n",
        "        # Calculate precision and recall\n",
        "        precision = self.true_positives / (self.true_positives + self.false_positives + tf.keras.backend.epsilon())\n",
        "        recall = self.true_positives / (self.true_positives + self.false_negatives + tf.keras.backend.epsilon())\n",
        "\n",
        "        # Calculate F1 score per class\n",
        "        f1_per_class = 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "        # Return macro average\n",
        "        return tf.reduce_mean(f1_per_class)\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(tf.zeros_like(self.true_positives))\n",
        "        self.false_positives.assign(tf.zeros_like(self.false_positives))\n",
        "        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n",
        "\n",
        "def train_model(model, data, epochs=50, batch_size=32, output_dir='models'):\n",
        "    \"\"\"\n",
        "    Train the BFRB detection model\n",
        "\n",
        "    Args:\n",
        "        model: Compiled Keras model\n",
        "        data: Dictionary containing training and validation data\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size for training\n",
        "        output_dir: Directory to save model files\n",
        "\n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare callbacks\n",
        "    callbacks = [\n",
        "        # Early stopping to prevent overfitting\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Reduce learning rate when training plateaus\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Save best model\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(output_dir, 'best_model.keras'),  # Use .keras extension\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting model training...\")\n",
        "    history = model.fit(\n",
        "        x=data['X_train'],\n",
        "        y=data['y_train'],\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(data['X_val'], data['y_val']),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    model.save(os.path.join(output_dir, 'final_model.keras'))  # Use .keras extension\n",
        "\n",
        "    # Save training history\n",
        "    pd.DataFrame(history.history).to_csv(\n",
        "        os.path.join(output_dir, 'training_history.csv'),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(f\"Model training complete. Models saved to {output_dir}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def evaluate_model(model, data):\n",
        "    \"\"\"\n",
        "    Evaluate the BFRB detection model\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        data: Dictionary containing evaluation data\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"Evaluating model...\")\n",
        "\n",
        "    # Evaluate on validation data\n",
        "    results = model.evaluate(\n",
        "        data['X_val'],\n",
        "        data['y_val'],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Extract metrics\n",
        "    metrics = {}\n",
        "    for i, metric_name in enumerate(model.metrics_names):\n",
        "        metrics[metric_name] = results[i]\n",
        "\n",
        "    print(\"Evaluation metrics:\")\n",
        "    for name, value in metrics.items():\n",
        "        print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def predict_and_save(model, data, output_file):\n",
        "    \"\"\"\n",
        "    Generate predictions and save to file for competition submission\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        data: Dictionary containing test data\n",
        "        output_file: Path to save predictions\n",
        "    \"\"\"\n",
        "    print(\"Generating predictions...\")\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(data['X_test'])\n",
        "\n",
        "    # Extract binary and multiclass predictions\n",
        "    binary_preds = predictions[0]\n",
        "    multiclass_preds = predictions[1]\n",
        "\n",
        "    # Convert to binary predictions (0 or 1)\n",
        "    binary_classes = (binary_preds > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Get class with highest probability for multiclass\n",
        "    multiclass_classes = np.argmax(multiclass_preds, axis=1)\n",
        "\n",
        "    # Map predictions to gestures\n",
        "    # Assuming class 0 is non-target, and classes 1-18 are specific gestures\n",
        "    # This mapping would need to be adjusted based on your actual class encoding\n",
        "    gesture_map = {\n",
        "        0: \"non_target\",\n",
        "        # Add mappings for other gesture classes based on your encoding\n",
        "    }\n",
        "\n",
        "    # Create output dataframe\n",
        "    output_df = pd.DataFrame({\n",
        "        'sequence_id': data['sequence_ids'],\n",
        "        'is_target': binary_classes,\n",
        "        'gesture_class': multiclass_classes,\n",
        "        'gesture': [gesture_map.get(cls, f\"gesture_{cls}\") for cls in multiclass_classes]\n",
        "    })\n",
        "\n",
        "    # Save predictions\n",
        "    output_df.to_csv(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "def save_model(model, model_dir):\n",
        "    \"\"\"\n",
        "    Save the trained model in multiple formats\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save in Keras native format (recommended)\n",
        "    model.save(os.path.join(model_dir, 'model.keras'))\n",
        "\n",
        "    # Save as SavedModel using export (for TFLite/TFServing)\n",
        "    model.export(os.path.join(model_dir, 'savedmodel'))\n",
        "\n",
        "    # Save in H5 format for compatibility\n",
        "    model.save(os.path.join(model_dir, 'model.h5'))\n",
        "\n",
        "    # Save model architecture as JSON\n",
        "    model_json = model.to_json()\n",
        "    with open(os.path.join(model_dir, 'model_architecture.json'), 'w') as f:\n",
        "        f.write(model_json)\n",
        "\n",
        "    # Save model weights separately\n",
        "    model.save_weights(os.path.join(model_dir, 'model.weights.h5'))\n",
        "\n",
        "    print(f\"Model saved to {model_dir} in multiple formats\")\n",
        "\n",
        "def load_model(model_dir):\n",
        "    \"\"\"\n",
        "    Load a trained model from a directory\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading Keras native format first\n",
        "        model = tf.keras.models.load_model(os.path.join(model_dir, 'model.keras'))\n",
        "        print(f\"Loaded model from {model_dir} (Keras format)\")\n",
        "    except:\n",
        "        try:\n",
        "            # Try loading H5 format\n",
        "            model = tf.keras.models.load_model(os.path.join(model_dir, 'model.h5'))\n",
        "            print(f\"Loaded model from {model_dir} (H5 format)\")\n",
        "        except:\n",
        "            # Load architecture and weights separately\n",
        "            with open(os.path.join(model_dir, 'model_architecture.json'), 'r') as f:\n",
        "                model_json = f.read()\n",
        "\n",
        "            model = tf.keras.models.model_from_json(model_json)\n",
        "            model.load_weights(os.path.join(model_dir, 'model.weights.h5'))\n",
        "\n",
        "            # Recompile model (add your custom metrics here)\n",
        "            model.compile(\n",
        "                optimizer=Adam(learning_rate=1e-3),\n",
        "                loss={\n",
        "                    'binary_output': BinaryCrossentropy(),\n",
        "                    'multiclass_output': SparseCategoricalCrossentropy()\n",
        "                },\n",
        "                metrics={\n",
        "                    'binary_output': [F1Score()],\n",
        "                    'multiclass_output': ['accuracy']\n",
        "                }\n",
        "            )\n",
        "\n",
        "            print(f\"Loaded model from {model_dir} (architecture + weights)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "    # GPU and mixed precision configuration is done at the top of the script\n",
        "\n",
        "    # Define paths to data files\n",
        "    train_file = 'train.csv'\n",
        "    train_demo_file = 'train_demographics.csv'\n",
        "    test_file = 'test.csv'\n",
        "    test_demo_file = 'test_demographics.csv'\n",
        "\n",
        "    # Load and preprocess data\n",
        "    data = load_and_preprocess_data(\n",
        "        train_file,\n",
        "        train_demo_file,\n",
        "        test_file,\n",
        "        test_demo_file\n",
        "    )\n",
        "\n",
        "    # Build model\n",
        "    model = build_multimodal_cnn()\n",
        "\n",
        "    # Compile model with custom metrics\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-3),\n",
        "        loss={\n",
        "            'binary_output': BinaryCrossentropy(),\n",
        "            'multiclass_output': SparseCategoricalCrossentropy()\n",
        "        },\n",
        "        metrics={\n",
        "            'binary_output': [F1Score()],\n",
        "            'multiclass_output': ['accuracy', MacroF1Score(num_classes=19)]\n",
        "        },\n",
        "        # Equal weight for both tasks (binary and multiclass)\n",
        "        loss_weights={\n",
        "            'binary_output': 0.5,\n",
        "            'multiclass_output': 0.5\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Print model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train model if we have training data\n",
        "    if 'X_train' in data and 'y_train' in data:\n",
        "        model, history = train_model(\n",
        "            model,\n",
        "            data,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            output_dir='bfrb_model'\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        if 'X_val' in data and 'y_val' in data:\n",
        "            metrics = evaluate_model(model, data)\n",
        "\n",
        "    # Generate predictions if we have test data\n",
        "    if 'X_test' in data:\n",
        "        predict_and_save(model, data, 'bfrb_predictions.csv')\n",
        "\n",
        "    # Save model\n",
        "    save_model(model, 'bfrb_model')\n",
        "\n",
        "    print(\"BFRB detection pipeline complete\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"cmi-detect-behavior-with-sensor-data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./\")"
      ],
      "metadata": {
        "id": "Z6KuaEF7_nvF"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}